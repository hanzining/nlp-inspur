{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T07:48:38.709904Z",
     "start_time": "2021-01-29T07:48:37.735206Z"
    }
   },
   "outputs": [],
   "source": [
    "#coding='utf-8'impo#coding='utf-8' \n",
    "import csv \n",
    "import jieba \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T07:48:39.542299Z",
     "start_time": "2021-01-29T07:48:39.415979Z"
    }
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('2000_news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T06:45:36.676979Z",
     "start_time": "2021-01-13T06:45:36.670736Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2504"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T06:49:35.772854Z",
     "start_time": "2021-01-13T06:49:35.759642Z"
    }
   },
   "outputs": [],
   "source": [
    "data=data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T06:53:48.527722Z",
     "start_time": "2021-01-13T06:53:48.508199Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TASK_URL</th>\n",
       "      <th>NEWS_TITLE</th>\n",
       "      <th>NEWS_SOURCE</th>\n",
       "      <th>NEWS_TIME</th>\n",
       "      <th>NEWS_CONTENT</th>\n",
       "      <th>TASK_URL.1</th>\n",
       "      <th>COMP_NM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>http://www.jjckb.cn/2020-10/29/c_139474646.htm</td>\n",
       "      <td>天津港再添“一带一路”新航线</td>\n",
       "      <td>经济参考报</td>\n",
       "      <td>2020-10-29</td>\n",
       "      <td>10月27日，一艘货轮停靠天津港。天津港集团正式开通运营今年第四条“一带一路”新航线（无...</td>\n",
       "      <td>http://info.search.news.cn/getNews?keyword=天津港...</td>\n",
       "      <td>天津港</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>http://www.jjckb.cn/2020-07/31/c_139253792.htm</td>\n",
       "      <td>LPG企业参与期货渐成趋势 万华化学注册2万吨LPG期货仓单</td>\n",
       "      <td>经济参考网</td>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>今年以来，原油能化板块价格波动广受市场关注。3月底LPG期货、期权同步上市以来，LPG衍...</td>\n",
       "      <td>http://info.search.news.cn/getNews?keyword=万华化...</td>\n",
       "      <td>万华化学</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>http://www.jjckb.cn/2020-12/03/c_139560028.htm</td>\n",
       "      <td>耀世星辉与科大讯飞签署年度广告合同并履行第一阶段950万元订单</td>\n",
       "      <td>大众网</td>\n",
       "      <td>2020-12-03</td>\n",
       "      <td>2020年12月1日，耀世星辉（GSMG.US)宣布完成与科大讯飞《讯飞程序化广告购买合...</td>\n",
       "      <td>http://info.search.news.cn/getNews?keyword=科大讯...</td>\n",
       "      <td>科大讯飞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>http://www.jjckb.cn/2020-07/22/c_139231973.htm</td>\n",
       "      <td>用户、技术、品牌：三大优势助推奇安信登陆科创板</td>\n",
       "      <td>新华财经</td>\n",
       "      <td>2020-07-22</td>\n",
       "      <td>从设立到科创板上市，这家技术“硬核”公司只用了6年。7月22日，网络安全龙头企业奇安信科技集...</td>\n",
       "      <td>http://info.search.news.cn/getNews?keyword=奇安信...</td>\n",
       "      <td>奇安信</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>http://www.jjckb.cn/2019-04/19/c_137988962.htm</td>\n",
       "      <td>老凤祥突破股权固化僵局再出发</td>\n",
       "      <td>经济参考报</td>\n",
       "      <td>2019-04-19</td>\n",
       "      <td>“双百试点5个月，老凤祥做成了10多年来一直想做而没能做成的事——完成股权结构改革，而且...</td>\n",
       "      <td>http://info.search.news.cn/getNews?keyword=老凤祥...</td>\n",
       "      <td>老凤祥</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         TASK_URL  \\\n",
       "0  http://www.jjckb.cn/2020-10/29/c_139474646.htm   \n",
       "1  http://www.jjckb.cn/2020-07/31/c_139253792.htm   \n",
       "2  http://www.jjckb.cn/2020-12/03/c_139560028.htm   \n",
       "3  http://www.jjckb.cn/2020-07/22/c_139231973.htm   \n",
       "4  http://www.jjckb.cn/2019-04/19/c_137988962.htm   \n",
       "\n",
       "                        NEWS_TITLE NEWS_SOURCE   NEWS_TIME  \\\n",
       "0                   天津港再添“一带一路”新航线       经济参考报  2020-10-29   \n",
       "1   LPG企业参与期货渐成趋势 万华化学注册2万吨LPG期货仓单       经济参考网  2020-07-31   \n",
       "2  耀世星辉与科大讯飞签署年度广告合同并履行第一阶段950万元订单         大众网  2020-12-03   \n",
       "3          用户、技术、品牌：三大优势助推奇安信登陆科创板        新华财经  2020-07-22   \n",
       "4                   老凤祥突破股权固化僵局再出发       经济参考报  2019-04-19   \n",
       "\n",
       "                                        NEWS_CONTENT  \\\n",
       "0  　　10月27日，一艘货轮停靠天津港。天津港集团正式开通运营今年第四条“一带一路”新航线（无...   \n",
       "1  　　今年以来，原油能化板块价格波动广受市场关注。3月底LPG期货、期权同步上市以来，LPG衍...   \n",
       "2  　　2020年12月1日，耀世星辉（GSMG.US)宣布完成与科大讯飞《讯飞程序化广告购买合...   \n",
       "3  从设立到科创板上市，这家技术“硬核”公司只用了6年。7月22日，网络安全龙头企业奇安信科技集...   \n",
       "4  　　“双百试点5个月，老凤祥做成了10多年来一直想做而没能做成的事——完成股权结构改革，而且...   \n",
       "\n",
       "                                          TASK_URL.1 COMP_NM  \n",
       "0  http://info.search.news.cn/getNews?keyword=天津港...     天津港  \n",
       "1  http://info.search.news.cn/getNews?keyword=万华化...    万华化学  \n",
       "2  http://info.search.news.cn/getNews?keyword=科大讯...    科大讯飞  \n",
       "3  http://info.search.news.cn/getNews?keyword=奇安信...     奇安信  \n",
       "4  http://info.search.news.cn/getNews?keyword=老凤祥...     老凤祥  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()\n",
    "# len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T06:55:18.568012Z",
     "start_time": "2021-01-13T06:55:18.529094Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop duplicate\n",
    "df=data.drop_duplicates(['COMP_NM','NEWS_TITLE'], keep='first').drop_duplicates(['COMP_NM','NEWS_CONTENT'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T06:55:34.448593Z",
     "start_time": "2021-01-13T06:55:34.433479Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TASK_URL</th>\n",
       "      <th>NEWS_TITLE</th>\n",
       "      <th>NEWS_SOURCE</th>\n",
       "      <th>NEWS_TIME</th>\n",
       "      <th>NEWS_CONTENT</th>\n",
       "      <th>TASK_URL.1</th>\n",
       "      <th>COMP_NM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>http://www.jjckb.cn/2020-10/29/c_139474646.htm</td>\n",
       "      <td>天津港再添“一带一路”新航线</td>\n",
       "      <td>经济参考报</td>\n",
       "      <td>2020-10-29</td>\n",
       "      <td>10月27日，一艘货轮停靠天津港。天津港集团正式开通运营今年第四条“一带一路”新航线（无...</td>\n",
       "      <td>http://info.search.news.cn/getNews?keyword=天津港...</td>\n",
       "      <td>天津港</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>http://www.jjckb.cn/2020-07/31/c_139253792.htm</td>\n",
       "      <td>LPG企业参与期货渐成趋势 万华化学注册2万吨LPG期货仓单</td>\n",
       "      <td>经济参考网</td>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>今年以来，原油能化板块价格波动广受市场关注。3月底LPG期货、期权同步上市以来，LPG衍...</td>\n",
       "      <td>http://info.search.news.cn/getNews?keyword=万华化...</td>\n",
       "      <td>万华化学</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>http://www.jjckb.cn/2020-12/03/c_139560028.htm</td>\n",
       "      <td>耀世星辉与科大讯飞签署年度广告合同并履行第一阶段950万元订单</td>\n",
       "      <td>大众网</td>\n",
       "      <td>2020-12-03</td>\n",
       "      <td>2020年12月1日，耀世星辉（GSMG.US)宣布完成与科大讯飞《讯飞程序化广告购买合...</td>\n",
       "      <td>http://info.search.news.cn/getNews?keyword=科大讯...</td>\n",
       "      <td>科大讯飞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>http://www.jjckb.cn/2020-07/22/c_139231973.htm</td>\n",
       "      <td>用户、技术、品牌：三大优势助推奇安信登陆科创板</td>\n",
       "      <td>新华财经</td>\n",
       "      <td>2020-07-22</td>\n",
       "      <td>从设立到科创板上市，这家技术“硬核”公司只用了6年。7月22日，网络安全龙头企业奇安信科技集...</td>\n",
       "      <td>http://info.search.news.cn/getNews?keyword=奇安信...</td>\n",
       "      <td>奇安信</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>http://www.jjckb.cn/2019-04/19/c_137988962.htm</td>\n",
       "      <td>老凤祥突破股权固化僵局再出发</td>\n",
       "      <td>经济参考报</td>\n",
       "      <td>2019-04-19</td>\n",
       "      <td>“双百试点5个月，老凤祥做成了10多年来一直想做而没能做成的事——完成股权结构改革，而且...</td>\n",
       "      <td>http://info.search.news.cn/getNews?keyword=老凤祥...</td>\n",
       "      <td>老凤祥</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         TASK_URL  \\\n",
       "0  http://www.jjckb.cn/2020-10/29/c_139474646.htm   \n",
       "1  http://www.jjckb.cn/2020-07/31/c_139253792.htm   \n",
       "2  http://www.jjckb.cn/2020-12/03/c_139560028.htm   \n",
       "3  http://www.jjckb.cn/2020-07/22/c_139231973.htm   \n",
       "4  http://www.jjckb.cn/2019-04/19/c_137988962.htm   \n",
       "\n",
       "                        NEWS_TITLE NEWS_SOURCE   NEWS_TIME  \\\n",
       "0                   天津港再添“一带一路”新航线       经济参考报  2020-10-29   \n",
       "1   LPG企业参与期货渐成趋势 万华化学注册2万吨LPG期货仓单       经济参考网  2020-07-31   \n",
       "2  耀世星辉与科大讯飞签署年度广告合同并履行第一阶段950万元订单         大众网  2020-12-03   \n",
       "3          用户、技术、品牌：三大优势助推奇安信登陆科创板        新华财经  2020-07-22   \n",
       "4                   老凤祥突破股权固化僵局再出发       经济参考报  2019-04-19   \n",
       "\n",
       "                                        NEWS_CONTENT  \\\n",
       "0  　　10月27日，一艘货轮停靠天津港。天津港集团正式开通运营今年第四条“一带一路”新航线（无...   \n",
       "1  　　今年以来，原油能化板块价格波动广受市场关注。3月底LPG期货、期权同步上市以来，LPG衍...   \n",
       "2  　　2020年12月1日，耀世星辉（GSMG.US)宣布完成与科大讯飞《讯飞程序化广告购买合...   \n",
       "3  从设立到科创板上市，这家技术“硬核”公司只用了6年。7月22日，网络安全龙头企业奇安信科技集...   \n",
       "4  　　“双百试点5个月，老凤祥做成了10多年来一直想做而没能做成的事——完成股权结构改革，而且...   \n",
       "\n",
       "                                          TASK_URL.1 COMP_NM  \n",
       "0  http://info.search.news.cn/getNews?keyword=天津港...     天津港  \n",
       "1  http://info.search.news.cn/getNews?keyword=万华化...    万华化学  \n",
       "2  http://info.search.news.cn/getNews?keyword=科大讯...    科大讯飞  \n",
       "3  http://info.search.news.cn/getNews?keyword=奇安信...     奇安信  \n",
       "4  http://info.search.news.cn/getNews?keyword=老凤祥...     老凤祥  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T07:36:51.736949Z",
     "start_time": "2021-01-13T07:36:51.711982Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove stopwords and space etc.\n",
    "stopwords = {}.fromkeys([ line.rstrip() for line in open('Stopword.txt','r',encoding='utf8') ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T07:26:14.196755Z",
     "start_time": "2021-01-13T07:26:10.469395Z"
    }
   },
   "outputs": [],
   "source": [
    "jieba.load_userdict(\"my_dict.txt\")\n",
    "jieba.load_userdict(\"dict_pangu.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T07:42:12.753039Z",
     "start_time": "2021-01-13T07:42:12.734707Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['天津港.再添.一带一路.新航线',\n",
       " 'LPG.企业.参与.期货.渐成.趋势.万华化学.注册.万吨.LPG.期货.仓单',\n",
       " '耀世星辉.科大讯飞.签署.年度.广告.合同.履行.第一阶段.950.万元.订单',\n",
       " '用户.技术.品牌.三大.优势.助推.奇安信.登陆.科.创.板',\n",
       " '老凤祥.突破.股权.固化.僵局.出发',\n",
       " '国信证券.当前.股市.维持.坚定.看多.判断.重点关注.具备.全球.竞争力.制造企业',\n",
       " '歌尔股份.大涨.近.全日.成交.近.70.亿元',\n",
       " '启明星辰.成为.北京.大数据.行动计划.首批.合作.单位',\n",
       " '京东方.创新.日.走进.大华股份.创新.赋.智慧.生活',\n",
       " '次新股.中天火箭.上演.天.地板',\n",
       " '全球.气候.治理.需.各国.携手同心',\n",
       " '景津环保.董事长.总经理.姜.桂.廷.用户.需求.最大.科研.方向',\n",
       " '全球.气候.治理.需.各国.携手同心',\n",
       " 'ST安通.重整.计划.预计.年内.执行.完成.2020.年.业绩.实现.扭亏为盈',\n",
       " 'ST安通.重整.计划.预计.12.月.31.日前.完成.今年.预计.扭亏为盈',\n",
       " '全球.首款.横.置.速.湿式.双.离合.变速器.下线.长城汽车.新一代.动力.总成.正式发布',\n",
       " '江淮汽车.助力.北京.打赢.蓝天.保卫战.首批.100.台.帅.铃.i5.新能源.物流.车.圆满.交付',\n",
       " '开启.希望.人生.打造.粤.厨.新.风范.李.锦.记.希望.厨师.粤菜.师傅.班.在广州.启程',\n",
       " '金.隅.冀东水泥.重大.资产重组.成绩单.亮眼',\n",
       " '万兴科技.发布.10.款.创意.软件.新品.战略.控股.墨.刀.牵手.视觉中国',\n",
       " '青农商行.母.净利润.增长.81.中间业务.收入.增速.44.14',\n",
       " '益生股份.董事长.曹.积.生.十四五.期间.鸡.苗.年销量.将达.10.亿只',\n",
       " '研发.创新.引领.高质量.发展.振德医疗.拓宽.医疗.健康.版图',\n",
       " '长租公寓.连环.爆.雷.深陷.信用.危机',\n",
       " '铜峰电子.控股.股东.大宗交易.方式.增持.1714.万股.占总.04',\n",
       " '龙蟒佰利.增资.收购.东方.钪.业.并向.东方.钪.业.转让.荣.佳.钪.钒.股权',\n",
       " '奇安信.齐向东.85.中小型.金融机构.实战.攻防.演习.中.攻破',\n",
       " '奇安信.齐向东.内生.安全.框架.助力.一带一路.企业.构建.防御.体系',\n",
       " '奇安信.齐向东.工业.互联网.新时代.已经.到来',\n",
       " '担当.谱写.使命.\\u3000.奋斗.托起.梦想',\n",
       " '东软集团.红旗.E.HS9.搭载.东软.C.V2X.智能.天线.产品.实现.场景.驾驶.辅助.预警',\n",
       " '齐鲁粮油.质.为名.\\u3000.打造.新时代.农产品.区域.公共.品牌',\n",
       " '美盛文化.控股.股东.拟向.九州.国泰.转让.公司.股份',\n",
       " '众信旅游.王府井.免税.达成.战略合作',\n",
       " '新华.财经.抗击.疫情.民族.品牌.行动.大年三十.逆行者.尔康制药.全力.抗击.疫情',\n",
       " 'ST安泰.再度.承诺.2024.年底.前.解决.关联交易',\n",
       " '绿地控股.自有资金.按期.兑付.89.亿.到期.债券',\n",
       " '中国.证券网.上市公司.公司.聚焦.绿地控股.按期.足额.兑付.到期.债券.主动.降.负债.去.杠杆',\n",
       " '绿地控股.地产.基建.核心.产业.剑指.双.万亿',\n",
       " '赵.超.总裁.荣获.民建.参与.抗击.新冠肺炎.疫情.先进个人.荣誉称号',\n",
       " '安徽江淮汽车集团股份有限公司.党委书记.董事长.安进.创新.驱动.汽车业.转型.升级.源.动力',\n",
       " '上交所.ST金钰.实控人.作出.纪律处分',\n",
       " '全球.气候.治理.需.各国.携手同心',\n",
       " '小康股份.前.11.月.新能源.车.销量.同比增长.倍',\n",
       " '七个.交易日.斩获.六个.涨停板.\\u3000.小康股份.发布.风险.提示',\n",
       " '利好.不断.小康股份.新能源.发展.步入.快车道',\n",
       " '专业.为本.追梦.资.大.时代.专访.新华.基金.联席.董事长.翟.晨曦',\n",
       " '后.疫情.时代.新华联.酒店.业务.创新.经营.力促.业绩增长',\n",
       " '涉嫌.操纵.证券市场.\\u3000.中潜股份.实控人.遭.立案调查',\n",
       " '十月.惊喜.东方.童.斩获.中国.诚信经营.百佳.示范单位.三大.殊荣',\n",
       " '京东方.创新.日.走进.大华股份.创新.赋.智慧.生活',\n",
       " '天风证券.拟.非.公开发行.128.亿元',\n",
       " '爱.今世缘.江苏今世缘酒业股份有限公司.党委.五.爱.学.教.活动.巡礼',\n",
       " '江苏今世缘酒业股份有限公司.党委副书记.副总经理.倪.春.工匠精神.打造.品牌.核心竞争力',\n",
       " '江淮汽车.前.三季度.营收.突破.400.亿元.同比增长',\n",
       " '江淮汽车.三季度.预.公告.销量.增速.高于.行业.实现.扭亏为盈',\n",
       " '江淮汽车.标.世界.前沿.标准.体系建设.实现.企业.高质量.发展',\n",
       " '江淮汽车.月销量.同比增长.41',\n",
       " '大秦铁路.320.亿元.可转债.正式.发行.用于.收购.土地使用权.完善.路网.布局',\n",
       " '中金公司.伴随着.需求.向.原材料.涨价.2021.年.造纸.行业.有望.整体.迎来.向上.行情',\n",
       " '中金公司.逢低.吸纳.符合.产业升级.消费.升级.主线.优质.成长.个股',\n",
       " '中金公司.A股.首日.开.板.\\u3000.H股.微涨.89',\n",
       " '康缘药业.董事长.肖.伟.后.疫情.时代.应.加快.完善.中医药.防治.体系',\n",
       " '国药股份.尾盘.涨停.国药.集团.新.冠.灭活疫苗.阿联酋.获批.上市',\n",
       " '医药.经济.报.守.正.创新.仲.景.宛西.经典.名方.焕发.新.活力',\n",
       " '增设.矿泉水.业务.均瑶健康.打造.第二.成长.曲线',\n",
       " '龙净环保.2019.年.营收.109.亿.增.16',\n",
       " '科技创新.彰显.实力.龙净环保.2019.年.获.政府.补助.超.亿元',\n",
       " '龙净环保.业绩.创新高.非.电.非.气.业务.表现.抢眼',\n",
       " '万兴科技.发布.10.款.创意.软件.新品.战略.控股.墨.刀.牵手.视觉中国',\n",
       " '荣丰控股.拟.作价.亿元.转让.重庆.荣丰.100.股权.保利.重庆.公司',\n",
       " '连云港.税务.助推.教育培训.百花齐放.深挖.大学生就业.蓄水池',\n",
       " '业绩.重回.增长.尔康制药.前三季.净利.81.亿',\n",
       " '牧原股份.拟.牧.原.集团.合资.设立.财务.公司.注册资本.10.亿元',\n",
       " '地方政府.整体规划.需求.江淮汽车.出让.土地.预计.净收益.亿',\n",
       " '江淮汽车.继.昨日.涨停.今日.大幅.拉升',\n",
       " '江淮汽车.一宗.土地.收储.预计.实现.净收益.亿元',\n",
       " '江淮汽车.拟.投资.10.亿.布局.新能源.汽车零部件.建设',\n",
       " '大众.首轮.增资.40.亿.江淮汽车.进入.大众.时代',\n",
       " '江.汽.集团.混.改.落实.锤.大众.入.资.吹响.江淮汽车.迈向.世界一流.车.企.号角',\n",
       " '百信银行.发布.全新.信贷.服务.品牌.会.花',\n",
       " '德勤.报告.亚太地区.经济.恢复.基本.面向',\n",
       " '海信.AI.国潮.馆.落地.武汉.欢乐谷.社交.电视.联手.抖音.记录.美好生活',\n",
       " '小巨人.成都.新.朝阳.引.战.投.和邦生物.拿下.股权',\n",
       " '实控人.调查.\\u3000.央.企.客户.订单.受.影响.\\u3000.晶澳科技.连续.两个.交易日.跌停',\n",
       " '九.牧.登上.八达岭长城.匠心.守护.长城.古建.文化',\n",
       " '居住.更美好.我爱我家.荣获.中.房.榜.两大.奖项',\n",
       " '坚持.质量.增长.我爱我家.相.寓.荣获.长租公寓.优质.品牌.榜样.奖',\n",
       " '我爱我家.守护者.计划.发布.公益.力量.推动.行业.升级',\n",
       " '携手.AWS.探索.云.上.转型.我爱我家.线上.化.能力.升级',\n",
       " '我爱我家.延续.盈利.势头.第三季度.实现.净利润.亿.门店.数.达.3500.家',\n",
       " '光.储.充.一体.科士达.储能.新能源.想象力.放到.最大']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cut news title test\n",
    "a=[]\n",
    "for i in df.loc[:100,'NEWS_TITLE']:\n",
    "    seg_list = jieba.cut(i, HMM=False)\n",
    "    seg_list = [x for x in seg_list if x not in stopwords and x !=' ']\n",
    "    a.append(\".\".join(seg_list))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T07:49:23.323542Z",
     "start_time": "2021-01-13T07:49:23.255181Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10.月.27.日.一艘.货轮.停靠.天津港.天津港.集团.正式.开通.运营.今年.第四条.一带一路.新航线.无人机.照片.新华社发',\n",
       " '今年以来.原油.化.板块.价格.波动.广受.市场.关注.月底.LPG.期货.期权.同步.上市.以来.LPG.衍生品.市场.运行.平稳.逐渐.产业链.企业所.接受.当下.国内外市场.环境.复杂.市场.风险.加剧.情况下.越来越.实体.企业.参与.衍生品.避险.记者.了解到.月.29.日.LPG.期货.首批.交割.厂.库.之一.万华化学.烟台.石化.有限公司.注册.1000.手.共.万吨.卖方.仓单.便于.日后.参与.期货交易.交割.锁定.生产.利润.据了解.LPG.价格.波动.季节性.较为.明显.冬季.LPG.需求.旺盛.期货.PG11.合约.属于.旺季.合约.现货.月份.传统.淡季.期.现货.基.差.合理.淡旺季.价差.范围内.具有.合理性.近期.基.差.不断扩大.趋势.月.29.日.收盘.LPG.期货.2011.合约.结算价.3910.元.吨.基.差.约为.1000.元.吨.近期.基.差.逐步.扩大.吸引.更多.产业.企业.开展.期.现.套利.操作.引导.基.差.向.合理.区间.收敛.市场价格.波动.前景.不明朗.时期.产业.企业.也应.积极.利用.衍生.工具.对冲.价格.风险.相关.专家.说.在此.背景下.LPG.期货.期权.上市.以来.已有.万华化学.中联.油.产业链.企业.参与.交易.万华化学.采购部.专员.王.宗.宾.介绍.此前.公司.曾.参与.外.盘纸.货.交易.刚.上市.不久.国内.LPG.期货.公司.来说.是一项.新业务.目前.公司.积极.学习.了解.期货.业务.已.逐步.参与.国内.期货交易.未来.考虑.更多.参与.国内.期.现.套.保.此次.万华化学.注册.万吨.标准.仓单.相关.专家.指出.当前.1000.元.吨.基.差.行情.下.生产企业.期货市场.做.卖出.套.保.注册.卖出.仓单.未来.可以通过.平仓.实物.交割.方式.获益.对冲.月.价格.波动.风险.提前.注册.11.月.仓单.后.厂.库.企业.持有.标准.仓单.抵押.银行.获得.融资.可以利用.标准.仓单.充.抵.折抵.保证金.便于.直接.交割.途经.向.交割.买方.销售.公开.数据.LPG.期货.上市.以来.保持.平稳.运行.上半年.日均.成交量.持仓量.分别为.41.06.万手.56.万手.市场规模.流动性.较好.整个.能源行业.提供.衍生.避险.工具.公司.下游.民用.客户.套.保.需求.非常大.大商所.上市.LPG.期货.后.在海外.点.价.下游.客户.再利用.国内.LPG.期货.进行.套.保.这是.LPG.期货.上市.产业.带来.一大.利好.大型.LPG.生产企业.负责人.表示.企业.利用.期货市场.浙商.期货.有限公司董事长.胡.军.此前.中国化工.产业.期货.大会上.表示.目前.期.现.结合.业务.模式.已.非常.成熟.企业.可以利用.不同.现货.价格.不同.月份.期货价格.进行.基.差.贸易.综合利用.含权.贸易.互换.掉期.业务.模式.实现.风险.转移.据悉.我国.全球最大.LPG.进口国.国际贸易.中.采用.境外.CP.FEI.价格指数.定价.国内.期货市场.快速发展.未来.期货价格.基准.基.差点.价.业务.有望.丰富.现有.定价.模式.提高.中国.价格.国际市场.影响力.相信.越来越.国内.国外.企业.会.参与.我国.期货市场.促进.LPG.期货.成为.现货.定价.提供.参考.依据.优化.我国.LPG.定价.体制.后续.会.实时.关注.参与.期货市场.服务.LPG.市场.健康.稳健.发展.王.宗.宾.说',\n",
       " '2020.年.12.月.日.耀世星辉.GSMG.US.宣布.完成.科大讯飞.讯.飞.程序化.广告.购买.合同.签署.服务.周期.执行.至.2021.年.12.月.31.日.目前.双方.已.履行.本合同.中.10.月.11.月.共计.950.万元.广告.执行.单.耀世星辉.确认.收到.科大讯飞.支付.第一笔.450.万元.人民币.收入.预计.未来.一年.合同.将为.耀世星辉.带来.超过.5000.万元.人民币.以上.广告.收入.科大讯飞.SZ.002230.亚太地区.知名.智能.语音.人工智能.上市.企业.自.成立以来.长期从事.语音.语言.自然语言.理解.机器.学习.推理.自主学习.核心技术.研究.保持.国际.前沿技术.水平.积极.推动.人工智能.产品研发.行业.应用.落地.致力.人工智能.建设.美好世界.作为.国内.领先.平台.型.内容.信息服务.内容.电商.公司.耀世星辉.借助.自身.优质.内容.广泛传播.力.悦.享.视频.平台.流量.聚合.效应.全.媒体.平台.资源优势.科大讯飞.品牌.产品.提供.信息.推广.服务.科大讯飞.长期以来.密切合作.耀世星辉.信息.智慧.服务.领域.建立了.良好.客户.口碑.品牌影响力.目前.旗下.内容.电商.平台.悦.享.视频.APP.装机量.已.突破.亿.日.活.超.700.万.优质.广泛.用户.流量.将使.耀世星辉.广告商.价值.越来越.大.预计.2020.年.全年.悦.享.视频.线上.广告.收入.实现.同比.50.以上.强劲.增长.耀世星辉.耀世星辉.专注于.优质.生活.领域.持续.内容.科技.时尚.社交.娱乐.线上.新.零售.融于.移动.互联网.新.文娱.产业.中.已.成长.聚合.优质.生活.内容.赋.新.消费.中国.新.文娱.创新.企业.2020.年.月.19.日.在美国.纳斯达克.NASDAQ.GSMG.上市.耀世星辉.内容.平台.电商.应用.技术.产业.为一体.拥有.悦.享.视频.APP.悦.享.商城.悦.系列.节目.集群.网络.综艺.网络.电视剧.短.视频.IP.矩阵.互动.直播.在线游戏.社交.电商.悦.享.会员.付费.体系.创新.平台.技术.产品.数字.经济.新.文娱.产业园.在内.业务.生态.体系.耀世星辉.将以.悦.享.为核心.产业.生态.布局.完成.自身.业务.品牌.战略.全面.升级.正在.实现.向.全球.范围.产业化.拓展.免责声明.此文.内容.本网站.刊发.转载.企业宣传.资讯.仅代表.作者.个人观点.与本网.无关.仅供.读者.参考.请自行.核实.相关内容',\n",
       " '设立.科.创.板.上市.这家.技术.硬核.公司.只用了.年.月.22.日.网络安全.龙头企业.奇安信.科技集团.正式.挂牌.科.创.板.上市.引发.市场.高度.关注.记者.梳理.发现.用户.规模.技术优势.品牌.忠诚度.构成了.这家.企业.重要.优势.此次.奇安信.原计划.募.资.45.亿元.实际.募.资.达.57.19.亿元.超.募.约.27.资金.投入.新.赛道.相关.项目.中.包括.基于.未来.网络.形态.安全.攻防.系统.建设.云和.大数据.安全防护.管理.运营.中心.工业.互联网安全.服务中心.涵盖.网络安全.行业.多个.重点.前沿.领域.奇安信.招股书.资料.透露.庞大.用户.规模.这家.公司.重要.优势.目前.奇安信.客户.已.覆盖.绝大多数.中央政府.部门.中央直属.企业.银行.大量.政企.单位.成为.国家.网络安全.保障.背后.重要.支撑.用户.信任.公司.产品.具有较强.不可.替代性.领先.前沿技术.能力.也是.奇安信.鲜明特点.奇安信.董事长.齐向东.告诉记者.5G.新.基建.数字化.转型.工业.互联网.发展.态势.网络安全.场景.进入.多元化.发展期.网络安全.技术.进入.升级换代.转折期.网络安全.需求.进入.爆炸式.增长期.形成了.创新.网络安全.技术.产品.新.赛道.为了更好.研发.新.赛道.产品.奇安信.重点建设.四大.研发.平台.安全产品.所需.共性.核心.能力.平台.化.标准化.在此基础上.研发.新.赛道.产品.其实.新产品.已经.逐步.成为.网络安全.行业.主.赛道.数据.显示.奇安信.新.赛道.产品.收入.近三年.复合.增长率.均.超过.100.2019.年.占.公司.总收入.超过.40.基于.技术优势.产生.强大.品牌影响力.同样.奇安信.高速成长.重要.推力.2014.年.心脏.滴血.漏洞.影响.三分之二.互联网.软件.几乎.每年.都会.发生.各类.网络安全.威胁.事件.近年来.奇安信.高额.投入.建立了.覆盖全国.网络安全.应急.响应.体系.数据.显示.2019.年.奇安信.共.处置.超过.2000.网络安全.应急.事件.实战.解决.客户.网络安全.问题.赢得了.客户.高度.信任.提升.老.用户.复.购.率.树立起.行业.良好.口碑.市场规模.来看.中国.网络空间.研究院.编写.世界.互联网.发展.报告.2019.显示.中国互联网.发展.水平.仅次于.美国.排名.全球.第二.庞大.互联网.市场.催生.几何.数量级.增长.网.安.市场.包括.奇安信.在内.网.安.企业.提供.广阔.发展空间.不少.业内人士.认为.经济.下行.压力.加大.叠加.新冠肺炎.疫情.黑天鹅.之时.奇安信.强势.登陆.科.创.板.推动.企业.自身.向.更高.层面.跃升.也为.国内.网.安.产业.资本市场.注入.一针.强心剂.上述.业内人士.指出.网络安全.国家安全.重要.战略.支撑.对比.美国.发达国家.中国网.安.产业.规模.比较.经济总量.尤其是.数字.经济.中.占.很低.IDC.数据.显示.2019.年.中国.网络安全.IT.投入.中.占.只有.84.美国.已经.达到.78.奇安信.作为.行业龙头.进一步.带动.中国.网络安全.行业.零.小规模.零散.化.同质化.迈向.高质量.发展.仍然.离不开.国家.政策.支持.行业.共识.齐向东.说.奇安信.目标.建成.全球.第一.网络安全.公司.上市.只是.第一步.过去.奋斗.实现了.上市.上市.是为了.未来.更好地.奋斗',\n",
       " '双百.试点.个月.老凤祥.做成.10.多年来.一直.想做.没能.做成.事.完成.股权结构.改革.实现了.平稳过渡.老凤祥.股份有限公司.以下.简称.老凤祥.股份.董事长.石.力.华.经济参考报.记者.表示.股权.固化.僵局.老凤祥.历史.遗留问题.也是.近些年.套.老凤祥.脖子上.一套.枷锁.一直.探寻.突破口.近日.记者.走进了.国企改革.双百.行动.试点企业.老凤祥.深入.了解.拥有.171.年.历史.企业.发展.情况.老凤祥.股份.领导班子.上海市.黄浦区.国资委.深入探讨.中.记者.发现了.百年.老字号.改革.基因.双百.行动.赋予.改革.契机.一拍即合.试点.几个月.时间内.便.完成.股权.改革.自身.企业发展.打开了.局面.也为.面临.类似.问题.国有企业.提供.经验.接下来.老凤祥.还将.加快.制定.配套改革.方案.探索.职业经理人.制度.新.股权激励.机制.人才.凤翔.计划.不断.改革.中.寻求.突破.老凤祥.纽约.第五.大道.旗舰店.资料.照片.股权.固化.迫切.寻求.改革.出路.建立.1848.年.老凤祥.170.余年.中华.老字号.因为.历史悠久.也有.自己的.遗留问题.股权.固化.问题.便是.十余年.困扰.老凤祥.头号.问题.这一.问题.缘起.20.世纪.九十年代.末.作为.老凤祥.股份.最.核心.业务.板块.下属.企业.上海.老凤祥.有限公司.设立.职工.持股会.职工.现金.历年.工资.结余.形式.出资.入股.后.实施.自然人.经营者.技术骨干.持股.截至.2018.年.月.股权.改革.前.职工.持股会.自然人.合计.持有.老凤祥.有限公司.21.99.股权.职工.持股会.持有.15.99.自然人.持有.当时.职工.持股.尤其是.经营.技术骨干.持股.经营.技术骨干.收益.企业.效益.密切.挂钩.极大地.增强.经营.技术骨干.积极性.主观能动性.企业.近二十年.快速.跨越式发展.注入.强大.动力.后劲.石.力.华.说.近年来.公司.高速发展.缺乏.完善.股权.退出.机制.职工.持股.导致.股权.固化.问题.逐渐.显现.石.力.华.指出.企业.资金.需求.越来越.大.资金.成本.压力.渐增.老凤祥.股份.作为.上市公司.受制于.老凤祥.有限公司.股权结构.问题.难以.市场.融资.直接.用于.有限公司.发展.需要.2009.年.为确保.上市公司.重组.顺利进行.职工股.股权.暂停.进退.目前.70.以上.持股.经营者.超过.退休年龄.股权.相对.固化.制约.企业发展.上市公司.中小.股东.一直.寄希望于.老凤祥.有限公司.经营.骨干.更多.结合.股份公司.层面.总体.发展.发挥.更大.作用.老凤祥.有限公司.迫切需要.引进.战略.投资者.实现.股权.有序.流动.解决.股权.股东.架构.方面.存在.历史.遗留问题.打破.股权.固化.僵局.实施.有效.激励.发挥.经营.技术骨干.核心作用.真正.企业.实现.命运.共存.利益.共享.推进.公司.转型.升级.十余年.时间.里.老凤祥.有限公司.一直.探索.股权.改革.路径.设计方案.寻求.出路.一直.没有找到.突破口.老凤祥.主业.处在.一个.完全.竞争.环境.没有.资源.市场.优势.竞争对手.中.90.以上.民营企业.机制.体制.灵活.不.变革.只有.死路一条.石.力.华.言辞.中.处处.体现出.老凤祥.改革.迫切.诉求.他说.老凤祥.2009.年.谋划.进行.混合.所有制.改革.但由于.各方面.条件.限制.改革.难以.顺利.推进.打破.僵局.双百.行动.助力.股权结构.改革.正当.老凤祥.变革.之.路上.苦苦.探索.之时.国务院国资委.部署.双百.行动.二者.一拍即合.说干就干.在接到.区.国资委.申报.双百.试点.消息.后.公司.连夜.召开会议.拟定.申请.试点.改革方案.老凤祥.股份有限公司.党委书记.副董事长.杨.奕.回忆道.股权.改革.涉及.上市公司.敏感.信息.涉及.持股.职工.切身利益.社会.关注度.高.必须要.系统.谋划.规范.推进.上海市.黄浦区.委.区政府.高度重视.老凤祥.有限公司.职工.持股会.股权转让.改革.明确提出.企业.未来.战略.发展.出发.把握.关键环节.稳定.妥善.一次性.彻底解决.股权.问题.老凤祥.股份.高质量.发展.创造条件.上海市.黄浦区.国资委.主任.沈.丹.娜.说.期间.公司.股权.沿革.进行了.全面.梳理.理清.股权.变动.脉络.明晰.股权.明确.权属.做到.持股会.260.名.成员.44.名.自然人.经营者.出资人.持股.情况.股权.数额.没有任何.争议.做到.及时.披露.信息.改革.全程.公开透明.具体操作.过程中.原有.股权.退出.战略.投资者.引进.完全.市场规律.办事.真正.改革.过程.交给.市场.公司.职工股.股权转让.不是.单纯.退出.而是要.股权转让.实现.强强联合.优势互补.石.力.华.说.公司.股权.受让方.选择.上.不是.简单.考虑.PE.价格.坚持.四个.是否.是否.具有.战略.发展.眼光.是否.具有.国内外.资本运作.经验.是否.具有.携手.进退.共同发展.诚意.是否.具有.强大.融资.能力.资金.实力.多个.维度.综合.考虑.筛选.战略.合作伙伴.最终.公司.选定.中国.国.新.控股有限公司.全资.子公司.国.新.控股.上海.有限公司.以下.简称.国.新.上海.公司.作为.战略.合作伙伴.之后.国.新.上海.公司.统筹协调.发起.设立.总规模.30.亿.老凤祥.专项基金.收购.职工.持股会.自然人.持有.21.99.股份.转让.对价.老凤祥.有限公司.审计.2017.年度.净利润.基准.运用.PE.估值.方法.确定.受让.价格.元.注册资本.人民币.60.元.含税.前期.细致.准备工作.职工.持股会.会员大会.自然人.持股.大会上.股权转让.方案.分别.99.94.100.获得.高票.退股.之后.保障.骨干.员工.积极性.股权转让.过程中.国.新.方面.提议.符合.相关法律法规.监管.政策.前提下.部分.原.持有.老凤祥.股份.经营.技术骨干.认购.老凤祥.专项基金.间接.持股.老凤祥.有限.目的是.较为.宽松.利益.捆绑.机制.保障.公司.持续发展.最终.各方.协商一致.并经.老凤祥.股份.股东大会.同意.38.名.经营.技术骨干.所得.股权.转让款.50.参与.老凤祥.专项基金.投资.老凤祥.骨干.持股.以外.基金.份额.国.新.上海.公司.遴选.战略.投资者.进一步.推动.资源.协同.至此.困扰.老凤祥.股份.十余年.股权.固化.问题.得到.解决.个月.时间.完成了.公司.股权结构.改革.实现了.平稳过渡.出乎.所有人.预料.石.力.华.说.如果没有.双百.行动.提供.改革.机会.各级.国资委.全力支持.这一.改革.难度.大得多.轻装上阵.凤翔.计划.深入.推进.捋.股权结构.后.公司.迎来了.不错.业绩.最新.业绩.报告显示.2018.年.老凤祥.股份.全年.实现.营业收入.437.亿元.同比增长.98.完成.利润.21.亿元.同比增长.89.经营规模.经济效益.再次.刷新.历史.最好.水平.只是.老凤祥.轻装上阵.出发.提供.良好.契机.石.力.华.说.老凤祥.还在.按照.双百.行动.要求.酝酿.更多.改革.配套方案.继续.推进.后续.改革.老凤祥.专项基金.中长期.持有.老凤祥.有限公司.股权.条件成熟.时.公司.按照.混合.所有制经济.发展.要求.符合.相关法律法规.监管部门.要求.前提下.探索.推进.新.股权激励.机制.稳固.核心.团队.激活.企业发展.内生.动力.石.力.华.说.石.力.华.透露.积极.快速.推进.职业经理人.制度.建立起.合同制.管理办法.打破.终身制.目前.已经.请.国际知名.人力资源.公司.老凤祥.做.职业经理人.制度改革.方案.今年.月份.准备.选择.三至.五家.下属.企业.进行.试点.2020.年.条件成熟.情况下.预计.至少.有一半.下属.企业.建立起.职业经理人.制度.外界.关心.人才.更迭.问题.杨.奕.坦言.目前.老凤祥.存在.关键.岗位.年龄.偏大.现象.顺利实现.人才.更迭.妥善处理.培养.年轻人.留住.精英.骨干.业务.骨干.关系.已经.纳入.今后.改革.重点.杨.奕.表示.目前.老凤祥.正在.实施.凤翔.计划.完善.人才.引入.激励机制.薪酬.年龄.等方面.力争.突破.既有.限制.具体情况.具体分析.实现.完全.市场化.管理机制.老凤祥.抓住.双百.行动.改革.契机.争取.形成.更多.试点经验.敢想.敢做.敢.突破.精神.实现.企业.更好.更快.更强.发展.石.力.华.说',\n",
       " '中证网.讯.记者.赵.中昊.国信证券.策略.团队.12.月.15.日.发布.研.报.表示.经济.复苏.盈利.回升.目前.已经成为.市场.共识.月份.以后.全球.资本市场.最大.交易.主题.无疑.经济.复苏.而在.交易.经济.复苏.范式.中.核心.问题是.两个.一是.盈利.回升.幅度.持续性.二.货币.收紧.时间.这两点.决定了.交易.复苏.节奏.盈利.回升.幅度.持续性.国信证券.认为.本轮.经济.复苏.2020.年.下半年.不论是.时间.上.幅度.上.来看.当前.时点.PPI.代表.复苏.指标.仍然.处在.回升.初期.而非.中后期.PPI.回升.至少.持续.2021.年.一季度.末.盈利.回升.逻辑.短期内.无法.证伪.这轮.复苏.中.盈利.上行.很强.超.预期.可能性.货币政策.看.关键.PPI.CPI.相对.变化.企业.盈利.影响.大.货币政策.影响.后者.国信证券.认为.本轮.复苏.更.类似.2016.2017.年.往后.看.PPI.会.快速.回升.CPI.没有.上行.压力.因此.很难.出现.因.通胀.预期.导致.货币政策.全面.收紧.本轮.复苏.中.海外.经济.复苏.弹性.和政策.刺激.力度.更大.国信证券.当前.股市.依然.维持.坚定.看多.判断.行业.结构.上看.家电.全球.产业链.企业.会.更加.受益.本轮.全球性.经济.复苏.重点关注.具备.全球.竞争力.制造企业',\n",
       " '歌尔股份.002241.日.高开高.走.盘中.大幅.拉升.截至.收盘.该股.涨.72.报.40.03.元.成交.69.亿元.消息.面上.北京时间.12.月.日.晚间.苹果.官方.微.信.公众.号.上.发布了.新品.头戴式.耳机.AirPods.Max.音质.音效.外观设计.等均.不俗.表现.售价.549.美元.4399.元.Apple.品牌.首款.头戴.耳机.AirPods.系列.最.高端产品.12.月.15.日.正式.上市.此前.媒体报道.这款.耳机.是由.歌尔股份.独家.代工.上证.报.记者.向.接近.公司.人士.求证.获得.默认.中金公司.指出.市场.反响.官网.预订.情况.看.AirPods.Max.表现.有机会.超出.此前.市场.预期.歌.作为.零组件.整机.主要.供应商.将会.受益.继.AirPods.后.歌尔股份.2020.年.下半年.接连.突破.AirPods.Pro.AirPods.Max.HomePod.mini.组装.产能.良.率.爬坡.顺利.体现出.强劲.竞争力.未来.苹果.更多.新品.及非.苹.客户.表现.更加.值得期待.机构.表示.海外.Occlus.quest.表现.来看.Facebook.硬件.销售.应用.生态.构建.均.非常.成功.Facebook.示范.效应.5G.趋势.下.预计.明年.ARVR.迎来.更多.巨头.进入.硬件.应用.运营商.共促.ARVR.市场繁荣.歌尔股份.重点.受益.预计.游戏机.零部件.明后.两年.也将.带来.显著.增量',\n",
       " '月.23.日.北京市.经济.信息化.局.开展了.北京.大数据.行动计划.首批.数据.合作.单位.签约仪式.启明星辰.集团.18.家.社会.机构.或其.代表.主体.签署.数据.合作.框架.协议.启明星辰.信息技术.集团股份有限公司.总裁.严.立.应邀出席.并签署.合作协议.北京.大数据.行动计划.2018.年.正式启动.北京市.深入.贯彻落实.党中央.国务院关于.推动.大数据.发展.决策.部署.重大.举措.计划.若干个.三年计划.滚动.实施.建成.完备.大数据.产业.生态.体系.北京市.大数据.整体.发展.水平.达到.国际.领先.严.立.表示.集团.成功.入选.首批.18.家.北京.大数据.行动计划.名单.国家.启明星辰.集团.数据安全.保障.服务.独立.安全.运营.战略.布局.认可.鼓励.数据安全.需要.网络安全.系统安全.业务.安全.多重.因素.共同.保障.集团.将为.用户.完善.数据.安全.汇聚.共享.持续.提供数据.安全解决方案.保障.服务.独立.安全.运营.中心.专业.安全.视角.平衡.用户.方.建设.方.信息化.安全.建设.助力.北京市.网络安全.信息化建设.启明星辰.集团.独立.安全.运营.中心.提供.集.产品.平台.服务.团队.为一体.综合.安全.数控.能力.作为.专业.安全.厂商.集团.第三方.专业.安全.视角.介入.网络安全.业务.设计.更好地.平衡.信息化建设.风险.防控.2017.年.12.月.成都.安全.运营.中心.建立.2018.年.11.月.青海省.运营.中心.落成.不到.一年.时间.启明星辰.集团.建成.筹划.建设.运营.中心.近.20.安全.运营.业务.完全.得到了.各地区.用户.认可.今后.启明星辰.集团.大力.配合.北京市.经济.信息化.局.进一步.引入.优质.社会.力量.充分发挥.出.社会.数据.价值.社会.机构.活力.加强.政务.数据.社会.数据.融合.应用.真正.释放.数据.红利.构建.高质量.可持续.数字.生态',\n",
       " '日前.京东方.创新.日.走进.大华股份.围绕.智慧.交通.智慧.办公.智慧.零售.日常生活中.熟悉.不同.场景.BOE.京东方.全方位.展示了.相关.领域.物.联网.创新.技术.解决方案.人们.更.智慧.高效.生活方式.充满.期待.活动.现场.BOE.京东方.智慧.交通.解决方案.吸引了.许多.参观者.驻足.体验.个人.出行.方面.BOE.京东方.带来了.智慧.座舱.产品.解决方案.智能.HUD.抬头.显示.产品.车速.导航.行车.信息.虚拟.呈现在.挡风玻璃.前.极大.提升.驾驶.体验.安全性.采用.柔性.OLED.技术.车尾灯.形态.自由.弯曲.更.酷炫.车灯.设计.成为.可能.公共交通.方面.BOE.京东方.宽.温.条形.屏.实现.全天候.无间断.稳定.显示.能为.乘客.实时.提供.交通.出行.信息.可以播放.图片.视频.多媒体.广告.交通.领域.BOE.京东方.推出了.透明.A.柱.OLED.内置.后视镜.墨水.屏.公交.站牌.电子.纸.双面.屏.拉手.智能.调光.玻璃.智慧.交通.创新.产品.解决方案.BOE.京东方.基于.人工智能.技术.推出.超.监控.客流.分析.解决方案.智慧.交通.领域.也有.广阔.应用.前景.超.监控.解决方案.智能.补充.图像.像素.细节.特征.监控.视频.中.模糊不清.的人.进行.放大.清晰化.处理.更.精准.识别.规范.交通.行为.智能.客流.分析.解决方案.热区.统计.方式.采集.各路.段.交通流量.排队.状况.交通.数据.持续.优化.交通管理.方式.提升.出行.体验.这次.创新.日.活动.BOE.京东方.带来.大.尺寸.会议.白板.智能.会议桌.牌.电子.门牌.智慧.办公.解决方案.智能.货架.大.尺寸.超高.清高.亮.广告.机.智慧.零售.解决方案.动态.心电.记录仪.无.创.参数.检测仪.移动.健康.解决方案.也让.现场.观众.感受到.科技.赋.之下.智慧.生活.新面貌.作为.全球.创新型.物.联网.企业.BOE.京东方.正.显示.技术.传感.技术.人工智能.算法.融入.万千.场景.物.联网.创新.解决方案.点亮.智慧.生活.完',\n",
       " '次新股.中天火箭.28.日.上演.天.地板.当日.早盘.中天火箭.涨停.价.开盘.随后.抛盘.集中.涌现.迅即.跌停.截至.收盘.公司.股价.封死.跌停.报收.77.02.元.跌停板.上.封.单.万手.全天.成交.51.亿元.中天火箭.上市.以来.走势.抢眼.该股.月.25.日.上市.发行价.每股.12.94.元.上市.首日.上涨.44.后.连续.走出.16.一字.涨停板.股价.一路.升至.85.58.元.10.月.25.日.晚间.中天火箭.发布.三季报.显示.今年前.三季度.实现.营业收入.46.亿元.同比增长.26.85.母.净利润.7290.58.万元.同比增长.23.64.基本.每股收益.63.元.今年以来.新股.上市.节奏.加快.上市.后.表现.出现.分化.态势.部分.个股.走出.多个.涨停板.也有.部分.新股.上市.次日.上市.首日.开.板.厦门银行.10.月.27.日.上市.首日.开.板.28.日.开盘.后.不久.跌停.北.元.集团.10.月.20.日.上市.当日.涨停.价.14.64.元.股.开盘.盘中.打开.涨停板.股价.涨幅.一度.缩.窄.至.25.86.之后.股价.有所.回升.中天火箭.同日.上市交易.中.谷物.流.上市.首日.盘中.打开.涨停板.收盘.封住.涨停.次日.股价.跌停.近期.上市.新股.中.中.岩.大地.海象.新.材.多股.上市.次日.开.板.仅.收获.一个.涨停板.前海.开源.基金.首席.经济学家.杨.德龙.表示.新股.密集.发行.加上.今年.二级市场.走弱.影响.新股.上市.后.表现.目前.注册.制.下.新股发行.采取.市场化.定价.机制.一些.科.创.板.创业板.新股.发行.价格.相对.较高.导致.新.收益.不断.下降',\n",
       " '12.月.12.日.正值.全球.应对.气候变化.巴黎.协定.达成.五周年.之际.2020.年.气候.雄心.峰会.视频.方式.举行.明年.26.届.气候变化.大会.带来.新.推动力.世界各国.致力于.新冠肺炎.疫情.中.恢复.经济.重振.全球.气候.治理.建设.更加.绿色.环境.将是.一项.重要.任务.翻看.巴黎.协定.签订.五年.成绩单.让人.有喜有忧.联合国.环境.发展署.统计.显示.2019.年.全球.温室.气体.排放.总量.创下了.591.亿吨.二氧化碳.当量.历史.新高.全球.二氧化碳.排放.总量.近三年.呈.增长.之势.只是在.疫情.导致.出行.受限.工业.活动.迅速.放缓.情况下.二氧化碳.总.排放量.下降了.世界气象组织.统计.目前.大气.中.温室.气体.浓度.已.破纪录.后.疫情.时代.全球.气候.治理.面临.更为.艰巨.任务.2020.年.原本.实现.温室.气体.2010.年.排放.水平.减少.45.目标.收官.之年.疫情.全球.气候.治理.进程.全面.放缓.要在.2050.年.实现.净.零排放.目标.时间.并不.充裕.国际社会.需.同心.一致.加强.合作.五年前.各国.领导人.推动.达成.应对.气候变化.巴黎.协定.实施过程.中.国际社会.广泛支持.参与.但也.遇到.不小.逆风.美国.作为.全球.累计.排放.温室.气体.最多.国家.气候变化.问题.上却.极度.消极.退出.协定.2019.年.美国.温室.气体.净.排放量.仍.高于.2016.年.水平.无法.兑现.此前.做出.承诺.作为.全球.人均.温室.气体.排放量.最高.国家.之一.澳大利亚.减排.态度.十分.消极.去年.至.今年年初.山火.更是.排放.亿吨.二氧化碳.可喜.的是.不少.国家.已经.认识到.问题.重要性.正在.正.准备.采取.紧急行动.传递.出.积极.信号.中国.宣布.提高.国家.自主.贡献.力度.力争.2030.年前.二氧化碳.排放.达到.峰值.单位.国内生产总值.二氧化碳.排放.2005.年.下降.65.以上.努力争取.2060.年前.实现.碳.中和.欧盟.27.成员国.推出.2030.年.气候.目标.计划.2030.年前.1990.年.温室.气体.排放量.减少.55.先前.40.目标.大幅提高.加拿大.宣布.增强版.气候.计划.计划.2030.年.排放量.降到.2005.年.水平.32.至.40.并在.2050.年.实现.净.零排放.英国.宣布.计划.2030.年.温室.气体.排放量.1990.年.至少.降低.68.美国.当选.总统.拜登.表示.美国.将在.明年.重返.巴黎.协定.疫情.延后.各国.减排.行动.提供.机遇.疫情.触发.人与自然.关系.深刻反思.经济.刺激.计划.应.成为.实现.绿色.复苏.契机.各国.共同努力.加快.创新.扩大.清洁.科技.市场.企业.层面.看.应.致力于.产品.设计.制造.使用.回收.全.环节.实现.碳.中和.发达国家.应.做出.实质性.承诺.切实.加大.向.发展中国家.提供.资金.技术.能力建设.支持.疫情.中.真正.实现.绿色.复苏.大幅度降低.温室.气体.排放.减缓.气候变化.带来.负面影响.各国政府.疫情.后.复苏.进程.中.投资.气候.保护.行动.并在.26.届.联合国.气候变化.大会上.提升.国家.自主.贡献.目标.承诺.实现.净.零排放.仍.有望.全球.碳.排放量.降至.基本.兑现.℃.温控.目标.应对.全球.气候变化.挑战.全球.各国.共同.责任.各国.需.形成.各尽所能.气候.治理.新.体系.秉持.人类.命运.共同体.理念.携手.应对.更.绿色.更具.弹性.发展道路.才是真正.高质量.可持续发展.道路']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cut news body test\n",
    "b=[]\n",
    "for i in df.loc[:10,'NEWS_CONTENT']:\n",
    "    seg_list = jieba.cut(i, HMM=False)\n",
    "    seg_list = [x for x in seg_list if x not in stopwords and x not in [' ', u'\\u3000',u'\\r\\n']]\n",
    "    b.append(\",\".join(seg_list))\n",
    "b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T06:39:13.082446Z",
     "start_time": "2021-01-29T06:39:13.074142Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\u3000\\u300012月12日，正值全球应对气候变化的《巴黎协定》达成五周年之际，2020年气候雄心峰会以视频方式举行，为明年第26届气候变化大会带来新的推动力。世界各国在致力于从新冠肺炎疫情中恢复经济的同时，重振全球气候治理，建设更加绿色的环境将是一项重要任务。\\r\\n\\u3000\\u3000翻看《巴黎协定》签订五年来的成绩单，让人有喜有忧。联合国环境发展署的统计显示，2019年全球温室气体排放总量创下了591亿吨二氧化碳当量的历史新高；全球二氧化碳排放总量近三年都呈增长之势，只是在疫情导致出行受限、工业活动迅速放缓的情况下，二氧化碳总排放量才下降了7%。然而，据世界气象组织的统计，目前大气中的温室气体浓度已破纪录。\\r\\n\\u3000\\u3000后疫情时代，全球气候治理面临更为艰巨的任务。2020年原本是实现温室气体较2010年排放水平减少45%目标的收官之年，但疫情使全球气候治理进程全面放缓。要在2050年实现净零排放的目标，时间并不充裕，国际社会需同心一致，加强合作。\\r\\n\\u3000\\u3000五年前，各国领导人推动达成应对气候变化的《巴黎协定》。在实施过程中，国际社会广泛支持和参与，但也遇到了不小的“逆风”。美国作为全球累计排放温室气体最多的国家，在气候变化问题上却极度消极，甚至退出协定。2019年，美国的温室气体净排放量仍高于2016年的水平，无法兑现其此前做出的承诺。作为全球人均温室气体排放量最高的国家之一，澳大利亚的减排态度也十分消极，去年至今年年初的山火更是多排放了4亿吨二氧化碳。\\r\\n\\u3000\\u3000可喜的是，不少国家已经认识到问题的重要性，正在或正准备采取紧急行动，传递出积极的信号。\\r\\n\\u3000\\u3000中国宣布将提高国家自主贡献力度，力争2030年前二氧化碳排放达到峰值，单位国内生产总值二氧化碳排放比2005年下降65%以上，努力争取2060年前实现碳中和。欧盟27个成员国推出“2030年气候目标计划”，在2030年前比1990年温室气体排放量减少55%，较先前40%的目标大幅提高。加拿大宣布“增强版气候计划”，计划到2030年将排放量降到2005年水平的32%至40%，并在2050年实现净零排放。英国宣布，计划到2030年温室气体排放量比1990年至少降低68%。美国当选总统拜登则表示美国将在明年重返《巴黎协定》。\\r\\n\\u3000\\u3000疫情延后了各国减排行动，但同时也提供了机遇。一方面，疫情触发对人与自然关系的深刻反思；另一方面，经济刺激计划应成为实现绿色复苏的契机。\\r\\n\\u3000\\u3000各国通过共同努力可以加快创新，扩大清洁科技市场。从企业层面看，应致力于在产品及其设计、制造、使用、回收全环节实现碳中和。发达国家应做出实质性承诺，切实加大向发展中国家提供资金、技术、能力建设支持。\\r\\n\\u3000\\u3000从疫情中真正实现绿色复苏，可以大幅度降低温室气体排放，并减缓气候变化带来的负面影响。如果各国政府在疫情后的复苏进程中投资气候保护行动，并在第26届联合国气候变化大会上提升各自的国家自主贡献目标，承诺实现“净零排放”，仍有望将全球碳排放量降至基本兑现“2℃温控目标”。\\r\\n\\u3000\\u3000应对全球气候变化的挑战，是全球各国共同的责任。各国需形成各尽所能的气候治理新体系，秉持人类命运共同体理念，携手应对。更绿色、更具弹性的发展道路，才是真正高质量、可持续发展的道路。'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[10,'NEWS_CONTENT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T07:20:34.635445Z",
     "start_time": "2021-01-29T07:20:34.632217Z"
    }
   },
   "outputs": [],
   "source": [
    "a='一度有望执掌阿里巴巴(Alibaba)的顶级高管之一周一遭到降职，此前爆出一起个人丑闻。根据英国《金融时报》看到的一份内部通知，公司调查显示，阿里巴巴旗下关键电子商务部门——天猫(Tmall)和淘宝(Taobao)——现年34岁的总裁蒋凡“因个人家庭问题处理不当，引发严重舆论危机，给公司声誉造成重大影响”。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T07:46:06.039702Z",
     "start_time": "2021-01-29T07:46:06.035132Z"
    }
   },
   "outputs": [],
   "source": [
    "data.loc[0,'NEWS_CONTENT']=a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T07:48:58.289895Z",
     "start_time": "2021-01-29T07:48:47.260404Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/cz/s5_j_1cn4c1crst9v4kt45_m0000gn/T/jieba.cache\n",
      "Loading model cost 0.761 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "天津港 positive\n",
      "万华化学 positive\n",
      "科大讯飞 positive\n",
      "奇安信 positive\n",
      "老凤祥 positive\n",
      "国信证券 positive\n",
      "歌尔股份 positive\n",
      "启明星辰 positive\n",
      "大华股份 positive\n",
      "中天火箭 positive\n",
      "华纺股份 positive\n",
      "景津环保 positive\n",
      "广东惠云钛业股份有限公司 positive\n",
      "ST安通 positive\n",
      "ST安通 positive\n",
      "ST辅仁 positive\n",
      "ST辅仁 positive\n",
      "新 希 望 positive\n",
      "冀东水泥 positive\n",
      "视觉中国 positive\n"
     ]
    }
   ],
   "source": [
    "from cnsenti import Sentiment\n",
    "for i in range(20):\n",
    "    senti = Sentiment(pos='dict/pos.txt',  #正面词典txt文件相对路径\n",
    "                      neg='dict/neg.txt',  #负面词典txt文件相对路径\n",
    "                      merge=True,             #融合cnsenti自带词典和用户导入的自定义词典\n",
    "                      encoding='utf-8')      #两txt均为utf-8编码\n",
    "\n",
    "    test_text=data.loc[i,'NEWS_CONTENT']\n",
    "    result = senti.sentiment_calculate(test_text)\n",
    "    \n",
    "    if result['neg']>result['pos']:\n",
    "        print(data.loc[i,'COMP_NM'],'negative')\n",
    "    elif result['neg']<result['pos']:\n",
    "        print(data.loc[i,'COMP_NM'],'positive')\n",
    "    else: print(data.loc[i,'COMP_NM'],'neutral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T07:18:08.399635Z",
     "start_time": "2021-01-29T07:18:08.387827Z"
    }
   },
   "outputs": [],
   "source": [
    "class ChiSquare:\n",
    "    def __init__(self, doc_list, doc_labels):\n",
    "        self.total_data, self.total_pos_data, self.total_neg_data = {}, {}, {}\n",
    "        for i, doc in enumerate(doc_list):\n",
    "            if doc_labels[i] == 1:\n",
    "                for word in doc:\n",
    "                    self.total_pos_data[word] = self.total_pos_data.get(word, 0) + 1\n",
    "                    self.total_data[word] = self.total_data.get(word, 0) + 1\n",
    "            else:\n",
    "                for word in doc:\n",
    "                    self.total_neg_data[word] = self.total_neg_data.get(word, 0) + 1\n",
    "                    self.total_data[word] = self.total_data.get(word, 0) + 1\n",
    "\n",
    "        total_freq = sum(self.total_data.values())\n",
    "        total_pos_freq = sum(self.total_pos_data.values())\n",
    "        # total_neg_freq = sum(self.total_neg_data.values())\n",
    "\n",
    "        self.words = {}\n",
    "        for word, freq in self.total_data.items():\n",
    "            pos_score = self.__calculate(self.total_pos_data.get(word, 0), freq, total_pos_freq, total_freq)\n",
    "            # neg_score = self.__calculate(self.total_neg_data.get(word, 0), freq, total_neg_freq, total_freq)\n",
    "            self.words[word] = pos_score * 2\n",
    "\n",
    "    @staticmethod\n",
    "    def __calculate(n_ii, n_ix, n_xi, n_xx):\n",
    "        n_ii = n_ii\n",
    "        n_io = n_xi - n_ii\n",
    "        n_oi = n_ix - n_ii\n",
    "        n_oo = n_xx - n_ii - n_oi - n_io\n",
    "        return n_xx * (float((n_ii*n_oo - n_io*n_oi)**2) /\n",
    "                       ((n_ii + n_io) * (n_ii + n_oi) * (n_io + n_oo) * (n_oi + n_oo)))\n",
    "\n",
    "    def best_words(self, num, need_score=False):\n",
    "        words = sorted(self.words.items(), key=lambda word_pair: word_pair[1], reverse=True)\n",
    "        if need_score:\n",
    "            return [word for word in words[:num]]\n",
    "        else:\n",
    "            return [word[0] for word in words[:num]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T07:45:44.770545Z",
     "start_time": "2021-01-29T07:45:44.432524Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "import jieba\n",
    "import numpy as np\n",
    "from jieba import posseg\n",
    "# ################################################\n",
    "# classifier based on sentiment f_dict\n",
    "# ################################################\n",
    "class DictClassifier:\n",
    "    def __init__(self):\n",
    "        self.__root_filepath = \"f_dict/\"\n",
    "\n",
    "        jieba.load_userdict(\"f_dict/user.dict\")  # 准备分词词典\n",
    "\n",
    "        # 准备情感词典词典\n",
    "        self.__phrase_dict = self.__get_phrase_dict()\n",
    "        self.__positive_dict = self.__get_dict(self.__root_filepath + \"positive_dict.txt\")\n",
    "        self.__negative_dict = self.__get_dict(self.__root_filepath + \"negative_dict.txt\")\n",
    "        self.__conjunction_dict = self.__get_dict(self.__root_filepath + \"conjunction_dict.txt\")\n",
    "        self.__punctuation_dict = self.__get_dict(self.__root_filepath + \"punctuation_dict.txt\")\n",
    "        self.__adverb_dict = self.__get_dict(self.__root_filepath + \"adverb_dict.txt\")\n",
    "        self.__denial_dict = self.__get_dict(self.__root_filepath + \"denial_dict.txt\")\n",
    "\n",
    "    def classify(self, sentence):\n",
    "        return self.analyse_sentence(sentence)\n",
    "\n",
    "    def analysis_file(self, filepath_in, filepath_out, encoding=\"utf-8\", print_show=False, start=0, end=-1):\n",
    "        open(filepath_out, \"w\")\n",
    "        results = []\n",
    "\n",
    "        with open(filepath_in, \"r\", encoding=encoding) as f:\n",
    "            line_number = 0\n",
    "            for line in f:\n",
    "                # 控制分析的语料的开始位置（行数）\n",
    "                line_number += 1\n",
    "                if line_number < start:\n",
    "                    continue\n",
    "\n",
    "                results.append(self.analyse_sentence(line.strip(), filepath_out, print_show))\n",
    "\n",
    "                # 控制分析的语料的结束位置（行数）\n",
    "                if 0 < end <= line_number:\n",
    "                    break\n",
    "\n",
    "        return results\n",
    "\n",
    "    def analyse_sentence(self, sentence, runout_filepath=None, print_show=False):\n",
    "        # 情感分析整体数据结构\n",
    "        comment_analysis = {\"score\": 0}\n",
    "\n",
    "        # 将评论分句\n",
    "        the_clauses = self.__divide_sentence_into_clauses(sentence + \"%\")\n",
    "\n",
    "        # 对每分句进行情感分析\n",
    "        for i in range(len(the_clauses)):\n",
    "            # 情感分析子句的数据结构\n",
    "            sub_clause = self.__analyse_clause(the_clauses[i].replace(\"。\", \".\"), runout_filepath, print_show)\n",
    "\n",
    "            # 将子句分析的数据结果添加到整体数据结构中\n",
    "            comment_analysis[\"su-clause\" + str(i)] = sub_clause\n",
    "            comment_analysis['score'] += sub_clause['score']\n",
    "\n",
    "        if runout_filepath is not None:\n",
    "            # 将整句写进运行输出文件，以便复查\n",
    "            self.__write_runout_file(runout_filepath, \"\\n\" + sentence + '\\n')\n",
    "            # 将每个评论的每个分句的分析结果写进运行输出文件，以便复查\n",
    "            self.__output_analysis(comment_analysis, runout_filepath)\n",
    "            # 将每个评论的的整体分析结果写进运行输出文件，以便复查\n",
    "            self.__write_runout_file(runout_filepath, str(comment_analysis) + \"\\n\\n\\n\\n\")\n",
    "        if print_show:\n",
    "            print(\"\\n\" + sentence)\n",
    "            self.__output_analysis(comment_analysis)\n",
    "            print(comment_analysis, end=\"\\n\\n\\n\")\n",
    "\n",
    "        if comment_analysis[\"score\"] > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def __analyse_clause(self, the_clause, runout_filepath, print_show):\n",
    "        sub_clause = {\"score\": 0, \"positive\": [], \"negative\": [], \"conjunction\": [], \"punctuation\": [], \"pattern\": []}\n",
    "        seg_result = posseg.lcut(the_clause)\n",
    "\n",
    "        # 将分句及分词结果写进运行输出文件，以便复查\n",
    "        if runout_filepath is not None:\n",
    "            self.__write_runout_file(runout_filepath, the_clause + '\\n')\n",
    "            self.__write_runout_file(runout_filepath, str(seg_result) + '\\n')\n",
    "        if print_show:\n",
    "            print(the_clause)\n",
    "            print(seg_result)\n",
    "\n",
    "        # 判断句式：如果……就好了\n",
    "        judgement = self.__is_clause_pattern2(the_clause)\n",
    "        if judgement != \"\":\n",
    "            sub_clause[\"pattern\"].append(judgement)\n",
    "            sub_clause[\"score\"] -= judgement[\"value\"]\n",
    "            return sub_clause\n",
    "\n",
    "        # 判断句式：是…不是…\n",
    "        judgement = self.__is_clause_pattern1(the_clause)\n",
    "        if judgement != \"\":\n",
    "            sub_clause[\"pattern\"].append(judgement)\n",
    "            sub_clause[\"score\"] -= judgement[\"value\"]\n",
    "\n",
    "        # 判断句式：短语\n",
    "        judgement = self.__is_clause_pattern3(the_clause, seg_result)\n",
    "        if judgement != \"\":\n",
    "            sub_clause[\"score\"] += judgement[\"score\"]\n",
    "            if judgement[\"score\"] >= 0:\n",
    "                sub_clause[\"positive\"].append(judgement)\n",
    "            elif judgement[\"score\"] < 0:\n",
    "                sub_clause[\"negative\"].append(judgement)\n",
    "            match_result = judgement[\"key\"].split(\":\")[-1]\n",
    "            i = 0\n",
    "            while i < len(seg_result):\n",
    "                if seg_result[i].word in match_result:\n",
    "                    if i + 1 == len(seg_result) or seg_result[i + 1].word in match_result:\n",
    "                        del (seg_result[i])\n",
    "                        continue\n",
    "                i += 1\n",
    "\n",
    "        # 逐个分析分词\n",
    "        for i in range(len(seg_result)):\n",
    "            mark, result = self.__analyse_word(seg_result[i].word, seg_result, i)\n",
    "            if mark == 0:\n",
    "                continue\n",
    "            elif mark == 1:\n",
    "                sub_clause[\"conjunction\"].append(result)\n",
    "            elif mark == 2:\n",
    "                sub_clause[\"punctuation\"].append(result)\n",
    "            elif mark == 3:\n",
    "                sub_clause[\"positive\"].append(result)\n",
    "                sub_clause[\"score\"] += result[\"score\"]\n",
    "            elif mark == 4:\n",
    "                sub_clause[\"negative\"].append(result)\n",
    "                sub_clause[\"score\"] -= result[\"score\"]\n",
    "\n",
    "        # 综合连词的情感值\n",
    "        for a_conjunction in sub_clause[\"conjunction\"]:\n",
    "            sub_clause[\"score\"] *= a_conjunction[\"value\"]\n",
    "\n",
    "        # 综合标点符号的情感值\n",
    "        for a_punctuation in sub_clause[\"punctuation\"]:\n",
    "            sub_clause[\"score\"] *= a_punctuation[\"value\"]\n",
    "\n",
    "        return sub_clause\n",
    "\n",
    "    @staticmethod\n",
    "    def __is_clause_pattern2(the_clause):\n",
    "        # re_pattern = re.compile(r\".*(如果|要是|希望).+就[\\u4e00-\\u9fa5]+(好|完美)了\")\n",
    "        re_pattern = re.compile(r\".*(如果|要是|希望).+就[\\u4e00-\\u9fa5]*(好|完美)了\")\n",
    "        match = re_pattern.match(the_clause)\n",
    "        if match is not None:\n",
    "            pattern = {\"key\": \"如果…就好了\", \"value\": 1.0}\n",
    "            return pattern\n",
    "        return \"\"\n",
    "\n",
    "    def __is_clause_pattern3(self, the_clause, seg_result):\n",
    "        for a_phrase in self.__phrase_dict:\n",
    "            keys = a_phrase.keys()\n",
    "            to_compile = a_phrase[\"key\"].replace(\"……\", \"[\\u4e00-\\u9fa5]*\")\n",
    "\n",
    "            if \"start\" in keys:\n",
    "                to_compile = to_compile.replace(\"*\", \"{\" + a_phrase[\"start\"] + \",\" + a_phrase[\"end\"] + \"}\")\n",
    "            if \"head\" in keys:\n",
    "                to_compile = a_phrase[\"head\"] + to_compile\n",
    "\n",
    "            match = re.compile(to_compile).search(the_clause)\n",
    "            if match is not None:\n",
    "                can_continue = True\n",
    "                pos = [flag for word, flag in posseg.cut(match.group())]\n",
    "                if \"between_tag\" in keys:\n",
    "                    if a_phrase[\"between_tag\"] not in pos and len(pos) > 2:\n",
    "                        can_continue = False\n",
    "\n",
    "                if can_continue:\n",
    "                    for i in range(len(seg_result)):\n",
    "                        if seg_result[i].word in match.group():\n",
    "                            try:\n",
    "                                if seg_result[i + 1].word in match.group():\n",
    "                                    return self.__emotional_word_analysis(\n",
    "                                        a_phrase[\"key\"] + \":\" + match.group(), a_phrase[\"value\"],\n",
    "                                        [x for x, y in seg_result], i)\n",
    "                            except IndexError:\n",
    "                                return self.__emotional_word_analysis(\n",
    "                                    a_phrase[\"key\"] + \":\" + match.group(), a_phrase[\"value\"],\n",
    "                                    [x for x, y in seg_result], i)\n",
    "        return \"\"\n",
    "\n",
    "    def __analyse_word(self, the_word, seg_result=None, index=-1):\n",
    "        # 判断是否是连词\n",
    "        judgement = self.__is_word_conjunction(the_word)\n",
    "        if judgement != \"\":\n",
    "            return 1, judgement\n",
    "\n",
    "        # 判断是否是标点符号\n",
    "        judgement = self.__is_word_punctuation(the_word)\n",
    "        if judgement != \"\":\n",
    "            return 2, judgement\n",
    "\n",
    "        # 判断是否是正向情感词\n",
    "        judgement = self.__is_word_positive(the_word, seg_result, index)\n",
    "        if judgement != \"\":\n",
    "            return 3, judgement\n",
    "\n",
    "        # 判断是否是负向情感词\n",
    "        judgement = self.__is_word_negative(the_word, seg_result, index)\n",
    "        if judgement != \"\":\n",
    "            return 4, judgement\n",
    "\n",
    "        return 0, \"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def __is_clause_pattern1(the_clause):\n",
    "        re_pattern = re.compile(r\".*(要|选)的.+(送|给).*\")\n",
    "        match = re_pattern.match(the_clause)\n",
    "        if match is not None:\n",
    "            pattern = {\"key\": \"要的是…给的是…\", \"value\": 1}\n",
    "            return pattern\n",
    "        return \"\"\n",
    "\n",
    "    def __is_word_conjunction(self, the_word):\n",
    "        if the_word in self.__conjunction_dict:\n",
    "            conjunction = {\"key\": the_word, \"value\": self.__conjunction_dict[the_word]}\n",
    "            return conjunction\n",
    "        return \"\"\n",
    "\n",
    "    def __is_word_punctuation(self, the_word):\n",
    "        if the_word in self.__punctuation_dict:\n",
    "            punctuation = {\"key\": the_word, \"value\": self.__punctuation_dict[the_word]}\n",
    "            return punctuation\n",
    "        return \"\"\n",
    "\n",
    "    def __is_word_positive(self, the_word, seg_result, index):\n",
    "        # 判断分词是否在情感词典内\n",
    "        if the_word in self.__positive_dict:\n",
    "            # 在情感词典内，则构建一个以情感词为中心的字典数据结构\n",
    "            return self.__emotional_word_analysis(the_word, self.__positive_dict[the_word],\n",
    "                                                  [x for x, y in seg_result], index)\n",
    "        # 不在情感词典内，则返回空\n",
    "        return \"\"\n",
    "\n",
    "    def __is_word_negative(self, the_word, seg_result, index):\n",
    "        # 判断分词是否在情感词典内\n",
    "        if the_word in self.__negative_dict:\n",
    "            # 在情感词典内，则构建一个以情感词为中心的字典数据结构\n",
    "            return self.__emotional_word_analysis(the_word, self.__negative_dict[the_word],\n",
    "                                                  [x for x, y in seg_result], index)\n",
    "        # 不在情感词典内，则返回空\n",
    "        return \"\"\n",
    "\n",
    "    def __emotional_word_analysis(self, core_word, value, segments, index):\n",
    "        # 在情感词典内，则构建一个以情感词为中心的字典数据结构\n",
    "        orientation = {\"key\": core_word, \"adverb\": [], \"denial\": [], \"value\": value}\n",
    "        orientation_score = orientation[\"value\"]  # my_sentiment_dict[segment]\n",
    "\n",
    "        # 在三个前视窗内，判断是否有否定词、副词\n",
    "        view_window = index - 1\n",
    "        if view_window > -1:  # 无越界\n",
    "            # 判断前一个词是否是情感词\n",
    "            if segments[view_window] in self.__negative_dict or \\\n",
    "                            segments[view_window] in self.__positive_dict:\n",
    "                orientation['score'] = orientation_score\n",
    "                return orientation\n",
    "            # 判断是否是副词\n",
    "            if segments[view_window] in self.__adverb_dict:\n",
    "                # 构建副词字典数据结构\n",
    "                adverb = {\"key\": segments[view_window], \"position\": 1,\n",
    "                          \"value\": self.__adverb_dict[segments[view_window]]}\n",
    "                orientation[\"adverb\"].append(adverb)\n",
    "                orientation_score *= self.__adverb_dict[segments[view_window]]\n",
    "            # 判断是否是否定词\n",
    "            elif segments[view_window] in self.__denial_dict:\n",
    "                # 构建否定词字典数据结构\n",
    "                denial = {\"key\": segments[view_window], \"position\": 1,\n",
    "                          \"value\": self.__denial_dict[segments[view_window]]}\n",
    "                orientation[\"denial\"].append(denial)\n",
    "                orientation_score *= -1\n",
    "        view_window = index - 2\n",
    "        if view_window > -1:\n",
    "            # 判断前一个词是否是情感词\n",
    "            if segments[view_window] in self.__negative_dict or \\\n",
    "                            segments[view_window] in self.__positive_dict:\n",
    "                orientation['score'] = orientation_score\n",
    "                return orientation\n",
    "            if segments[view_window] in self.__adverb_dict:\n",
    "                adverb = {\"key\": segments[view_window], \"position\": 2,\n",
    "                          \"value\": self.__adverb_dict[segments[view_window]]}\n",
    "                orientation_score *= self.__adverb_dict[segments[view_window]]\n",
    "                orientation[\"adverb\"].insert(0, adverb)\n",
    "            elif segments[view_window] in self.__denial_dict:\n",
    "                denial = {\"key\": segments[view_window], \"position\": 2,\n",
    "                          \"value\": self.__denial_dict[segments[view_window]]}\n",
    "                orientation[\"denial\"].insert(0, denial)\n",
    "                orientation_score *= -1\n",
    "                # 判断是否是“不是很好”的结构（区别于“很不好”）\n",
    "                if len(orientation[\"adverb\"]) > 0:\n",
    "                    # 是，则引入调节阈值，0.3\n",
    "                    orientation_score *= 0.3\n",
    "        view_window = index - 3\n",
    "        if view_window > -1:\n",
    "            # 判断前一个词是否是情感词\n",
    "            if segments[view_window] in self.__negative_dict or segments[view_window] in self.__positive_dict:\n",
    "                orientation['score'] = orientation_score\n",
    "                return orientation\n",
    "            if segments[view_window] in self.__adverb_dict:\n",
    "                adverb = {\"key\": segments[view_window], \"position\": 3,\n",
    "                          \"value\": self.__adverb_dict[segments[view_window]]}\n",
    "                orientation_score *= self.__adverb_dict[segments[view_window]]\n",
    "                orientation[\"adverb\"].insert(0, adverb)\n",
    "            elif segments[view_window] in self.__denial_dict:\n",
    "                denial = {\"key\": segments[view_window], \"position\": 3,\n",
    "                          \"value\": self.__denial_dict[segments[view_window]]}\n",
    "                orientation[\"denial\"].insert(0, denial)\n",
    "                orientation_score *= -1\n",
    "                # 判断是否是“不是很好”的结构（区别于“很不好”）\n",
    "                if len(orientation[\"adverb\"]) > 0 and len(orientation[\"denial\"]) == 0:\n",
    "                    orientation_score *= 0.3\n",
    "        # 添加情感分析值。\n",
    "        orientation['score'] = orientation_score\n",
    "        # 返回的数据结构\n",
    "        return orientation\n",
    "\n",
    "    # 输出comment_analysis分析的数据结构结果\n",
    "    def __output_analysis(self, comment_analysis, runout_filepath=None):\n",
    "        output = \"Score:\" + str(comment_analysis[\"score\"]) + \"\\n\"\n",
    "\n",
    "        for i in range(len(comment_analysis) - 1):\n",
    "            output += \"Sub-clause\" + str(i) + \": \"\n",
    "            clause = comment_analysis[\"su-clause\" + str(i)]\n",
    "            if len(clause[\"conjunction\"]) > 0:\n",
    "                output += \"conjunction:\"\n",
    "                for punctuation in clause[\"conjunction\"]:\n",
    "                    output += punctuation[\"key\"] + \" \"\n",
    "            if len(clause[\"positive\"]) > 0:\n",
    "                output += \"positive:\"\n",
    "                for positive in clause[\"positive\"]:\n",
    "                    if len(positive[\"denial\"]) > 0:\n",
    "                        for denial in positive[\"denial\"]:\n",
    "                            output += denial[\"key\"] + str(denial[\"position\"]) + \"-\"\n",
    "                    if len(positive[\"adverb\"]) > 0:\n",
    "                        for adverb in positive[\"adverb\"]:\n",
    "                            output += adverb[\"key\"] + str(adverb[\"position\"]) + \"-\"\n",
    "                    output += positive[\"key\"] + \" \"\n",
    "            if len(clause[\"negative\"]) > 0:\n",
    "                output += \"negative:\"\n",
    "                for negative in clause[\"negative\"]:\n",
    "                    if len(negative[\"denial\"]) > 0:\n",
    "                        for denial in negative[\"denial\"]:\n",
    "                            output += denial[\"key\"] + str(denial[\"position\"]) + \"-\"\n",
    "                    if len(negative[\"adverb\"]) > 0:\n",
    "                        for adverb in negative[\"adverb\"]:\n",
    "                            output += adverb[\"key\"] + str(adverb[\"position\"]) + \"-\"\n",
    "                    output += negative[\"key\"] + \" \"\n",
    "            if len(clause[\"punctuation\"]) > 0:\n",
    "                output += \"punctuation:\"\n",
    "                for punctuation in clause[\"punctuation\"]:\n",
    "                    output += punctuation[\"key\"] + \" \"\n",
    "            if len(clause[\"pattern\"]) > 0:\n",
    "                output += \"pattern:\"\n",
    "                for pattern in clause[\"pattern\"]:\n",
    "                    output += pattern[\"key\"] + \" \"\n",
    "            # if clause[\"pattern\"] is not None:\n",
    "            #     output += \"pattern:\" + clause[\"pattern\"][\"key\"] + \" \"\n",
    "            output += \"\\n\"\n",
    "        if runout_filepath is not None:\n",
    "            self.__write_runout_file(runout_filepath, output)\n",
    "        else:\n",
    "            print(output)\n",
    "\n",
    "    def __divide_sentence_into_clauses(self, the_sentence):\n",
    "        the_clauses = self.__split_sentence(the_sentence)\n",
    "\n",
    "        # 识别“是……不是……”句式\n",
    "        pattern = re.compile(r\"([，、。%！；？?,!～~.… ]*)([\\u4e00-\\u9fa5]*?(要|选)\"\n",
    "                             r\"的.+(送|给)[\\u4e00-\\u9fa5]+?[，。！%；、？?,!～~.… ]+)\")\n",
    "        match = re.search(pattern, the_sentence.strip())\n",
    "        if match is not None and len(self.__split_sentence(match.group(2))) <= 2:\n",
    "            to_delete = []\n",
    "            for i in range(len(the_clauses)):\n",
    "                if the_clauses[i] in match.group(2):\n",
    "                    to_delete.append(i)\n",
    "            if len(to_delete) > 0:\n",
    "                for i in range(len(to_delete)):\n",
    "                    the_clauses.remove(the_clauses[to_delete[0]])\n",
    "                the_clauses.insert(to_delete[0], match.group(2))\n",
    "\n",
    "        # 识别“要是|如果……就好了”的假设句式\n",
    "        pattern = re.compile(r\"([，%。、！；？?,!～~.… ]*)([\\u4e00-\\u9fa5]*?(如果|要是|\"\n",
    "                             r\"希望).+就[\\u4e00-\\u9fa5]+(好|完美)了[，。；！%、？?,!～~.… ]+)\")\n",
    "        match = re.search(pattern, the_sentence.strip())\n",
    "        if match is not None and len(self.__split_sentence(match.group(2))) <= 3:\n",
    "            to_delete = []\n",
    "            for i in range(len(the_clauses)):\n",
    "                if the_clauses[i] in match.group(2):\n",
    "                    to_delete.append(i)\n",
    "            if len(to_delete) > 0:\n",
    "                for i in range(len(to_delete)):\n",
    "                    the_clauses.remove(the_clauses[to_delete[0]])\n",
    "                the_clauses.insert(to_delete[0], match.group(2))\n",
    "\n",
    "        the_clauses[-1] = the_clauses[-1][:-1]\n",
    "        return the_clauses\n",
    "\n",
    "    @staticmethod\n",
    "    def __split_sentence(sentence):\n",
    "        pattern = re.compile(\"[，。%、！!？?,；～~.… ]+\")\n",
    "\n",
    "        split_clauses = pattern.split(sentence.strip())\n",
    "        punctuations = pattern.findall(sentence.strip())\n",
    "        try:\n",
    "            split_clauses.remove(\"\")\n",
    "        except ValueError:\n",
    "            pass\n",
    "        punctuations.append(\"\")\n",
    "\n",
    "        clauses = [''.join(x) for x in zip(split_clauses, punctuations)]\n",
    "\n",
    "        return clauses\n",
    "\n",
    "    def __get_phrase_dict(self):\n",
    "        sentiment_dict = []\n",
    "        pattern = re.compile(r\"\\s+\")\n",
    "        with open(self.__root_filepath + \"phrase_dict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                a_phrase = {}\n",
    "                result = pattern.split(line.strip())\n",
    "                if len(result) >= 2:\n",
    "                    a_phrase[\"key\"] = result[0]\n",
    "                    a_phrase[\"value\"] = float(result[1])\n",
    "                    for i, a_split in enumerate(result):\n",
    "                        if i < 2:\n",
    "                            continue\n",
    "                        else:\n",
    "                            a, b = a_split.split(\":\")\n",
    "                            a_phrase[a] = b\n",
    "                    sentiment_dict.append(a_phrase)\n",
    "\n",
    "        return sentiment_dict\n",
    "\n",
    "    # 情感词典的构建\n",
    "    @staticmethod\n",
    "    def __get_dict(path, encoding=\"utf-8\"):\n",
    "        sentiment_dict = {}\n",
    "        pattern = re.compile(r\"\\s+\")\n",
    "        with open(path, encoding=encoding) as f:\n",
    "            for line in f:\n",
    "                result = pattern.split(line.strip())\n",
    "                if len(result) == 2:\n",
    "                    sentiment_dict[result[0]] = float(result[1])\n",
    "        return sentiment_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def __write_runout_file(path, info, encoding=\"utf-8\"):\n",
    "        with open(path, \"a\", encoding=encoding) as f:\n",
    "            f.write(\"%s\" % info)\n",
    "\n",
    "\n",
    "# ################################################\n",
    "# classifier based on K-Nearest Neighbours\n",
    "# ################################################\n",
    "class KNNClassifier:\n",
    "    def __init__(self, train_data, train_data_labels, k=3, best_words=None, stopwords=None):\n",
    "        self.__train_data_labels = []\n",
    "        self.__total_words = []\n",
    "        self.__k = k\n",
    "        self.__stopwords = stopwords\n",
    "        self.__train_data_vectors = None\n",
    "        self.__total_words_length = 0\n",
    "        self.train_num = 0\n",
    "        if train_data is not None:\n",
    "            self.__train(train_data, train_data_labels, best_words)\n",
    "\n",
    "    def set_k(self, k):\n",
    "        self.__k = k\n",
    "\n",
    "    def __doc2vector(self, doc):\n",
    "        the_vector = [0] * self.__total_words_length\n",
    "        for i in range(self.__total_words_length):\n",
    "            the_vector[i] = doc.count(self.__total_words[i])\n",
    "        length = sum(the_vector)\n",
    "        if length == 0:\n",
    "            return [0 for _ in the_vector]\n",
    "        return [i / length for i in the_vector]\n",
    "        # return the_vector\n",
    "\n",
    "    def __get_total_words(self, train_data, best_words):\n",
    "        if best_words is not None:\n",
    "            total_words = best_words[:]\n",
    "        else:\n",
    "            total_words = set()\n",
    "            for doc in train_data:\n",
    "                total_words |= set(doc)\n",
    "        if self.__stopwords:\n",
    "            with open(self.__stopwords, encoding=\"utf-8\") as sw_f:\n",
    "                for line in sw_f:\n",
    "                    if line.strip() in total_words:\n",
    "                        total_words.remove(line.strip())\n",
    "        return list(total_words)\n",
    "\n",
    "    @staticmethod\n",
    "    def __normalize(vectors):\n",
    "        min_values = vectors.min(axis=0)\n",
    "        max_values = vectors.max(axis=0)\n",
    "        ranges = max_values - min_values\n",
    "        m = vectors.shape[0]\n",
    "        norm_vectors = vectors - np.tile(min_values, (m, 1))\n",
    "        norm_vectors = norm_vectors / np.tile(ranges, (m, 1))\n",
    "        return norm_vectors\n",
    "\n",
    "    def __train(self, train_data, train_data_labels, best_words=None):\n",
    "        print(\"KNNClassifier is training ...... \")\n",
    "\n",
    "        self.__train_data_labels = train_data_labels[:]\n",
    "        self.__total_words = self.__get_total_words(train_data, best_words)\n",
    "        self.__total_words_length = len(self.__total_words)\n",
    "        vectors = []\n",
    "        for doc in train_data:\n",
    "            vectors.append(self.__doc2vector(doc))\n",
    "            self.train_num += 1\n",
    "\n",
    "        self.__train_data_vectors = np.array(vectors)\n",
    "        # self.__train_data_vectors = self.__normalize(np.array(vectors))\n",
    "\n",
    "        print(\"KNNClassifier trains over!\")\n",
    "\n",
    "    def __get_sorted_distances(self, input_data):\n",
    "        size = self.__train_data_vectors.shape\n",
    "        vector = self.__doc2vector(input_data)\n",
    "        input_data_vector = np.array(vector)\n",
    "        diff_mat = np.tile(input_data_vector, (size[0], 1)) - self.__train_data_vectors\n",
    "        sq_diff_mat = diff_mat ** 2\n",
    "        sq_distances = sq_diff_mat.sum(axis=1)\n",
    "        distances = sq_distances ** 0.5\n",
    "        sorted_distances = distances.argsort()\n",
    "\n",
    "        return sorted_distances\n",
    "\n",
    "    def classify(self, input_data):\n",
    "        if isinstance(self.__k, int):\n",
    "            return self.single_k_classify(input_data)\n",
    "        elif isinstance(self.__k, list):\n",
    "            return self.multiple_k_classify(input_data)\n",
    "        else:\n",
    "            print(\"Wrong k.\")\n",
    "\n",
    "    def multiple_k_classify(self, input_data):\n",
    "        # get the distance sorted list\n",
    "        sorted_distances = self.__get_sorted_distances(input_data)\n",
    "\n",
    "        # some variable\n",
    "        i = 0\n",
    "        # class_count[0] records the number of label \"0\"\n",
    "        # class_count[1] records the number of label \"1\"\n",
    "        class_count = [0, 0]\n",
    "        # final_record[0] records the number of label \"0\"\n",
    "        # final_record[1] records the number of label \"1\"\n",
    "        final_record = [0, 0]\n",
    "\n",
    "        # assert type(k) == list\n",
    "        assert type(self.__k) == list\n",
    "\n",
    "        for k in sorted(self.__k):\n",
    "            while i < k:\n",
    "                label = self.__train_data_labels[sorted_distances[i]]\n",
    "                class_count[label] += 1\n",
    "                i += 1\n",
    "            if class_count[0] > class_count[1]:\n",
    "                final_record[0] += 1\n",
    "            else:\n",
    "                final_record[1] += 1\n",
    "\n",
    "        if final_record[0] > final_record[1]:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def single_k_classify(self, input_data):\n",
    "        # get the distance sorted list\n",
    "        sorted_distances = self.__get_sorted_distances(input_data)\n",
    "\n",
    "        # some variable\n",
    "        i = 0\n",
    "        # class_count[0] records the number of label \"0\"\n",
    "        # class_count[1] records the number of label \"1\"\n",
    "        class_count = [0, 0]\n",
    "\n",
    "        while i < self.__k:\n",
    "            label = self.__train_data_labels[sorted_distances[i]]\n",
    "            class_count[label] += 1\n",
    "            i += 1\n",
    "\n",
    "        if class_count[0] > class_count[1]:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "\n",
    "# ################################################\n",
    "# classifier based on Naive bayes\n",
    "# ################################################\n",
    "class BayesClassifier:\n",
    "    def __init__(self, train_data, train_data_labels, best_words):\n",
    "        self._pos_word_p = {}\n",
    "        self._neg_word_p = {}\n",
    "        self._pos_p = 0.\n",
    "        self._neg_p = 1.\n",
    "        self._train(train_data, train_data_labels, best_words)\n",
    "\n",
    "    def _train(self, train_data, train_data_labels, best_words=None):\n",
    "        \"\"\"\n",
    "        this method is different from the the method self.train()\n",
    "        we use the training data, do some feature selection, then train,\n",
    "        get some import values\n",
    "        :param train_data:\n",
    "        :param train_data_labels:\n",
    "        :param best_words:\n",
    "        \"\"\"\n",
    "        print(\"BayesClassifier is training ...... \")\n",
    "\n",
    "        # get the frequency information of each word\n",
    "        total_pos_data, total_neg_data = {}, {}\n",
    "        total_pos_length, total_neg_length = 0, 0\n",
    "        total_word = set()\n",
    "        for i, doc in enumerate(train_data):\n",
    "            if train_data_labels[i] == 1:\n",
    "                for word in doc:\n",
    "                    if best_words is None or word in best_words:\n",
    "                        total_pos_data[word] = total_pos_data.get(word, 0) + 1\n",
    "                        total_pos_length += 1\n",
    "                        total_word.add(word)\n",
    "            else:\n",
    "                for word in doc:\n",
    "                    if best_words is None or word in best_words:\n",
    "                        total_neg_data[word] = total_neg_data.get(word, 0) + 1\n",
    "                        total_neg_length += 1\n",
    "                        total_word.add(word)\n",
    "        self._pos_p = total_pos_length / (total_pos_length + total_neg_length)\n",
    "        self._neg_p = total_neg_length / (total_pos_length + total_neg_length)\n",
    "\n",
    "        # get each word's probability\n",
    "        for word in total_word:\n",
    "            self._pos_word_p[word] = np.log(total_pos_data.get(word, 1e-100) / total_pos_length)\n",
    "            self._neg_word_p[word] = np.log(total_neg_data.get(word, 1e-100) / total_neg_length)\n",
    "\n",
    "        print(\"BayesClassifier trains over!\")\n",
    "\n",
    "    def classify(self, input_data):\n",
    "        \"\"\"\n",
    "        according to the input data, calculate the probability of the each class\n",
    "        :param input_data:\n",
    "        \"\"\"\n",
    "        pos_score = 0.\n",
    "        for word in input_data:\n",
    "            pos_score += self._pos_word_p.get(word, 0.)\n",
    "        pos_score += np.log(self._pos_p)\n",
    "\n",
    "        neg_score = 0.\n",
    "        for word in input_data:\n",
    "            neg_score += self._neg_word_p.get(word, 0.)\n",
    "        neg_score += np.log(self._neg_p)\n",
    "\n",
    "        if pos_score > neg_score:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "\n",
    "# ################################################\n",
    "# classifier based on Maximum Entropy\n",
    "# ################################################\n",
    "class MaxEntClassifier:\n",
    "    def __init__(self, max_iter=500):\n",
    "        self.feats = defaultdict(int)\n",
    "        self.labels = {0, 1}\n",
    "        self.weight = []\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def prob_weight(self, features, label):\n",
    "        weight = 0.0\n",
    "        for feature in features:\n",
    "            if (label, feature) in self.feats:\n",
    "                weight += self.weight[self.feats[(label, feature)]]\n",
    "        return np.exp(weight)\n",
    "\n",
    "    def calculate_probability(self, features):\n",
    "        weights = [(self.prob_weight(features, label), label) for label in self.labels]\n",
    "        try:\n",
    "            z = sum([weight for weight, label in weights])\n",
    "            prob = [(weight / z, label) for weight, label in weights]\n",
    "        except ZeroDivisionError:\n",
    "            return \"collapse\"\n",
    "        return prob\n",
    "\n",
    "    def convergence(self, last_weight):\n",
    "        for w1, w2 in zip(last_weight, self.weight):\n",
    "            if abs(w1 - w2) >= 0.001:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def train(self, train_data, train_data_labels, best_words=None):\n",
    "        print(\"MaxEntClassifier is training ...... \")\n",
    "\n",
    "        # init the parameters\n",
    "        train_data_length = len(train_data_labels)\n",
    "        if best_words is None:\n",
    "            for i in range(train_data_length):\n",
    "                for word in set(train_data[i]):\n",
    "                    self.feats[(train_data_labels[i], word)] += 1\n",
    "        else:\n",
    "            for i in range(train_data_length):\n",
    "                for word in set(train_data[i]):\n",
    "                    if word in best_words:\n",
    "                        self.feats[(train_data_labels[i], word)] += 1\n",
    "\n",
    "        the_max = max([len(record) - 1 for record in train_data])  # the_max param for GIS training algorithm\n",
    "        self.weight = [0.0] * len(self.feats)  # init weight for each feature\n",
    "        ep_empirical = [0.0] * len(self.feats)  # init the feature expectation on empirical distribution\n",
    "        for i, f in enumerate(self.feats):\n",
    "            ep_empirical[i] = self.feats[f] / train_data_length  # feature expectation on empirical distribution\n",
    "            self.feats[f] = i  # each feature function correspond to id\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            ep_model = [0.0] * len(self.feats)  # feature expectation on model distribution\n",
    "            for doc in train_data:\n",
    "                prob = self.calculate_probability(doc)  # calculate p(y|x)\n",
    "                if prob == \"collapse\":\n",
    "                    print(\"The program collapse. The iter number: %d.\" % (i + 1))\n",
    "                    return\n",
    "                for feature in doc:\n",
    "                    for weight, label in prob:\n",
    "                        if (label, feature) in self.feats:  # only focus on features from training data.\n",
    "                            idx = self.feats[(label, feature)]  # get feature id\n",
    "                            ep_model[idx] += weight * (1.0 / train_data_length)  # sum(1/N * f(y,x)*p(y|x)), p(x) = 1/N\n",
    "\n",
    "            last_weight = self.weight[:]\n",
    "            for j, win in enumerate(self.weight):\n",
    "                delta = 1.0 / the_max * np.log(ep_empirical[j] / ep_model[j])\n",
    "                self.weight[j] += delta  # update weight\n",
    "\n",
    "            # test if the algorithm is convergence\n",
    "            if self.convergence(last_weight):\n",
    "                print(\"The program convergence. The iter number: %d.\" % (i + 1))\n",
    "                break\n",
    "\n",
    "        print(\"MaxEntClassifier trains over!\")\n",
    "\n",
    "    def test(self, train_data, train_labels, best_words, test_data):\n",
    "        classify_results = []\n",
    "\n",
    "        # init the parameters\n",
    "        train_data_length = len(train_labels)\n",
    "        if best_words is None:\n",
    "            for i in range(train_data_length):\n",
    "                for word in set(train_data[i]):\n",
    "                    self.feats[(train_labels[i], word)] += 1\n",
    "        else:\n",
    "            for i in range(train_data_length):\n",
    "                for word in set(train_data[i]):\n",
    "                    if word in best_words:\n",
    "                        self.feats[(train_labels[i], word)] += 1\n",
    "\n",
    "        the_max = max([len(record) - 1 for record in train_data])  # the_max param for GIS training algorithm\n",
    "        self.weight = [0.0] * len(self.feats)  # init weight for each feature\n",
    "        ep_empirical = [0.0] * len(self.feats)  # init the feature expectation on empirical distribution\n",
    "        for i, f in enumerate(self.feats):\n",
    "            ep_empirical[i] = self.feats[f] / train_data_length  # feature expectation on empirical distribution\n",
    "            self.feats[f] = i  # each feature function correspond to id\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            print(\"MaxEntClassifier is training ...... \")\n",
    "\n",
    "            ep_model = [0.0] * len(self.feats)  # feature expectation on model distribution\n",
    "            for doc in train_data:\n",
    "                prob = self.calculate_probability(doc)  # calculate p(y|x)\n",
    "                if prob == \"collapse\":\n",
    "                    print(\"The program collapse. The iter number: %d.\" % (i + 1))\n",
    "                    return\n",
    "                for feature in doc:\n",
    "                    for weight, label in prob:\n",
    "                        if (label, feature) in self.feats:  # only focus on features from training data.\n",
    "                            idx = self.feats[(label, feature)]  # get feature id\n",
    "                            ep_model[idx] += weight * (1.0 / train_data_length)  # sum(1/N * f(y,x)*p(y|x)), p(x) = 1/N\n",
    "\n",
    "            last_weight = self.weight[:]\n",
    "            for j, win in enumerate(self.weight):\n",
    "                delta = 1.0 / the_max * np.log(ep_empirical[j] / ep_model[j])\n",
    "                self.weight[j] += delta  # update weight\n",
    "\n",
    "            print(\"MaxEntClassifier is testing ...\")\n",
    "            classify_labels = []\n",
    "            for data in test_data:\n",
    "                classify_labels.append(self.classify(data))\n",
    "            classify_results.append(classify_labels)\n",
    "\n",
    "            # test if the algorithm is convergence\n",
    "            if self.convergence(last_weight):\n",
    "                print(\"The program convergence. The iter number: %d.\" % (i + 1))\n",
    "                break\n",
    "\n",
    "        print(\"MaxEntClassifier trains over!\")\n",
    "\n",
    "        return classify_results\n",
    "\n",
    "    def classify(self, the_input_features):\n",
    "        prob = self.calculate_probability(the_input_features)\n",
    "        prob.sort(reverse=True)\n",
    "        if prob[0][0] > prob[1][0]:\n",
    "            return prob[0][1]\n",
    "        else:\n",
    "            return prob[1][1]\n",
    "\n",
    "\n",
    "# ################################################\n",
    "# classifier based on Support Vector Machine\n",
    "# ################################################\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "class SVMClassifier:\n",
    "    def __init__(self, train_data, train_labels, best_words, C):\n",
    "        train_data = np.array(train_data)\n",
    "        train_labels = np.array(train_labels)\n",
    "\n",
    "        self.best_words = best_words\n",
    "        self.clf = SVC(C=C)\n",
    "        self.__train(train_data, train_labels)\n",
    "\n",
    "    def words2vector(self, all_data):\n",
    "        vectors = []\n",
    "\n",
    "        best_words_index = {}\n",
    "        for i, word in enumerate(self.best_words):\n",
    "            best_words_index[word] = i\n",
    "\n",
    "        for data in all_data:\n",
    "            vector = [0 for x in range(len(self.best_words))]\n",
    "            for word in data:\n",
    "                i = best_words_index.get(word)\n",
    "                if i is not None:\n",
    "                    vector[i] = vector[i] + 1\n",
    "            vectors.append(vector)\n",
    "\n",
    "        vectors = np.array(vectors)\n",
    "        return vectors\n",
    "\n",
    "    def __train(self, train_data, train_labels):\n",
    "        print(\"SVMClassifier is training ...... \")\n",
    "\n",
    "        train_vectors = self.words2vector(train_data)\n",
    "\n",
    "        self.clf.fit(train_vectors, np.array(train_labels))\n",
    "\n",
    "        print(\"SVMClassifier trains over!\")\n",
    "\n",
    "    def classify(self, data):\n",
    "        vector = self.words2vector([data])\n",
    "\n",
    "        prediction = self.clf.predict(vector)\n",
    "\n",
    "        return prediction[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T07:16:13.719677Z",
     "start_time": "2021-01-29T07:16:13.715260Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T07:33:26.369556Z",
     "start_time": "2021-01-29T07:33:26.359139Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\u3000\\u3000近期，A股持仓震荡持续。12月13日，中金公司表示，在三季度市场偏弱的阶段，持续提示对四季度不宜悲观，“复苏交易为主线”。近期“顺周期”的“复苏交易”几乎成为一致预期，但市场风格转换趋弱，市场在连续三周小幅上涨后也出现了回调。\\r\\n\\u3000\\u3000中金公司认为市场风格转换较为纠结、整体表现不及预期的原因，可能有如下几个方面：一是，在近年成长风格持续跑赢市场后，成长风格的资金明显占主导，对周期性板块兴趣不大；二是，在近年的结构性牛市中大部分机构投资者取得较好收益，临近年底，大幅加仓周期股的意愿不足；三是，近年投资者结构持续向机构投资者转换，市场短期“跟风”类资金占比明显减小，周期拉涨的持续性相比之下明显减弱；四是，近期政策基调有朝进一步偏紧的方向演进，持续在传递“总闸门”、“防风险”的基调。\\r\\n\\u3000\\u3000综合来看，临近年底市场风险偏好可能偏弱，中金公司建议投资者在市场的变化中稳守基本面与估值底线，注重低估值、有正面催化等因素，逢低吸纳符合产业升级与消费升级主线的优质成长个股。'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[66,'NEWS_CONTENT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T07:45:47.366558Z",
     "start_time": "2021-01-29T07:45:47.282883Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "阿里巴巴(Alibaba)旗下平台通常占中国在线销售总额的三分之二，\n",
      "[pair('阿里巴巴', 'nrt'), pair('(', 'x'), pair('Alibaba', 'eng'), pair(')', 'x'), pair('旗下', 'n'), pair('平台', 'n'), pair('通常', 'd'), pair('占', 'v'), pair('中国', 'ns'), pair('在线', 'b'), pair('销售总额', 'n'), pair('的', 'uj'), pair('三分之二', 'm'), pair('，', 'x')]\n",
      "长期以来被视为中国经济健康状况的指标.\n",
      "[pair('长期以来', 'l'), pair('被', 'p'), pair('视为', 'v'), pair('中国', 'ns'), pair('经济', 'n'), pair('健康状况', 'n'), pair('的', 'uj'), pair('指标', 'n'), pair('.', 'x')]\n",
      "周四，\n",
      "[pair('周四', 't'), pair('，', 'x')]\n",
      "这个指标给出令人沮丧的诊断.\n",
      "[pair('这个', 'r'), pair('指标', 'n'), pair('给出', 'v'), pair('令人', 'nrt'), pair('沮丧', 'a'), pair('的', 'uj'), pair('诊断', 'v'), pair('.', 'x')]\n",
      "首席执行官张勇(Daniel \n",
      "[pair('首席', 'n'), pair('执行官', 'n'), pair('张勇', 'nr'), pair('(', 'x'), pair('Daniel', 'eng'), pair(' ', 'x')]\n",
      "Zhang)警告称，\n",
      "[pair('Zhang', 'eng'), pair(')', 'x'), pair('警告', 'n'), pair('称', 'v'), pair('，', 'x')]\n",
      "新型冠状病毒(COVID-19)疫情使中国陷入停摆，\n",
      "[pair('新型', 'b'), pair('冠状病毒', 'l'), pair('(', 'x'), pair('COVID', 'eng'), pair('-', 'x'), pair('19', 'm'), pair(')', 'x'), pair('疫情', 'n'), pair('使', 'v'), pair('中国', 'ns'), pair('陷入', 'v'), pair('停摆', 'v'), pair('，', 'x')]\n",
      "将会全面影响阿里巴巴的业务，\n",
      "[pair('将', 'd'), pair('会', 'v'), pair('全面', 'n'), pair('影响', 'vn'), pair('阿里巴巴', 'nrt'), pair('的', 'uj'), pair('业务', 'n'), pair('，', 'x')]\n",
      "即便他宣布去年最后一个季度净利润同比增长56%，\n",
      "[pair('即便', 'c'), pair('他', 'r'), pair('宣布', 'v'), pair('去年', 't'), pair('最后', 'f'), pair('一个', 'm'), pair('季度', 'n'), pair('净利润', 'n'), pair('同比', 'j'), pair('增长', 'v'), pair('56', 'm'), pair('%', 'x'), pair('，', 'x')]\n",
      "至520亿元人民币（合75亿美元）.\n",
      "[pair('至', 'p'), pair('520', 'm'), pair('亿元', 'm'), pair('人民币', 'n'), pair('（', 'x'), pair('合', 'n'), pair('75', 'm'), pair('亿美元', 'm'), pair('）', 'x'), pair('.', 'x')]\n",
      "\n",
      "阿里巴巴(Alibaba)旗下平台通常占中国在线销售总额的三分之二，长期以来被视为中国经济健康状况的指标。周四，这个指标给出令人沮丧的诊断。首席执行官张勇(Daniel Zhang)警告称，新型冠状病毒(COVID-19)疫情使中国陷入停摆，将会全面影响阿里巴巴的业务，即便他宣布去年最后一个季度净利润同比增长56%，至520亿元人民币（合75亿美元）。\n",
      "Score:1.0\n",
      "Sub-clause0: \n",
      "Sub-clause1: positive:经济 \n",
      "Sub-clause2: \n",
      "Sub-clause3: negative:沮丧 \n",
      "Sub-clause4: \n",
      "Sub-clause5: positive:称 \n",
      "Sub-clause6: \n",
      "Sub-clause7: \n",
      "Sub-clause8: \n",
      "Sub-clause9: \n",
      "\n",
      "{'score': 1.0, 'su-clause0': {'score': 0, 'positive': [], 'negative': [], 'conjunction': [], 'punctuation': [], 'pattern': []}, 'su-clause1': {'score': 1.0, 'positive': [{'key': '经济', 'adverb': [], 'denial': [], 'value': 1.0, 'score': 1.0}], 'negative': [], 'conjunction': [], 'punctuation': [], 'pattern': []}, 'su-clause2': {'score': 0, 'positive': [], 'negative': [], 'conjunction': [], 'punctuation': [], 'pattern': []}, 'su-clause3': {'score': -1.0, 'positive': [], 'negative': [{'key': '沮丧', 'adverb': [], 'denial': [], 'value': 1.0, 'score': 1.0}], 'conjunction': [], 'punctuation': [], 'pattern': []}, 'su-clause4': {'score': 0, 'positive': [], 'negative': [], 'conjunction': [], 'punctuation': [], 'pattern': []}, 'su-clause5': {'score': 1.0, 'positive': [{'key': '称', 'adverb': [], 'denial': [], 'value': 1.0, 'score': 1.0}], 'negative': [], 'conjunction': [], 'punctuation': [], 'pattern': []}, 'su-clause6': {'score': 0, 'positive': [], 'negative': [], 'conjunction': [], 'punctuation': [], 'pattern': []}, 'su-clause7': {'score': 0, 'positive': [], 'negative': [], 'conjunction': [], 'punctuation': [], 'pattern': []}, 'su-clause8': {'score': 0, 'positive': [], 'negative': [], 'conjunction': [], 'punctuation': [], 'pattern': []}, 'su-clause9': {'score': 0, 'positive': [], 'negative': [], 'conjunction': [], 'punctuation': [], 'pattern': []}}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = DictClassifier()\n",
    "f = 'test.txt'\n",
    "# a_sentence = \"一度有望执掌阿里巴巴(Alibaba)的顶级高管之一周一遭到降职，此前爆出一起个人丑闻，涉及他的妻子和在这家电商巨擘的平台上销售商品的一位网红。\"\n",
    "d.analysis_file(f,'out.txt',encoding=\"utf-8\", print_show=True, start=0, end=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Maximal Marginal Releuance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T07:32:26.626934Z",
     "start_time": "2021-02-02T07:32:26.561962Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T07:50:54.233987Z",
     "start_time": "2021-02-02T07:50:54.162813Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def calculateSimilarity(sentence, doc):  # 根据句子和句子，句子和文档的余弦相似度\n",
    "    if doc == []:\n",
    "        return 0\n",
    "    vocab = {}\n",
    "    for word in sentence.split():\n",
    "        vocab[word] = 0  # 生成所在句子的单词字典，值为0\n",
    " \n",
    "    docInOneSentence = ''\n",
    "    for t in doc:\n",
    "        docInOneSentence += (t + ' ')  # 所有剩余句子合并\n",
    "        for word in t.split():\n",
    "            vocab[word] = 0  # 所有剩余句子的单词字典，值为0\n",
    " \n",
    "    cv = CountVectorizer(vocabulary=vocab.keys())\n",
    " \n",
    "    docVector = cv.fit_transform([docInOneSentence])\n",
    "    sentenceVector = cv.fit_transform([sentence])\n",
    "    return cosine_similarity(docVector, sentenceVector)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T07:50:56.681801Z",
     "start_time": "2021-02-02T07:50:56.290895Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-355bca52c290>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmmr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# kurangkan dengan set summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msummarySet\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mmmr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcalculateSimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarySet\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 公式\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scores' is not defined"
     ]
    }
   ],
   "source": [
    "n = 3\n",
    "alpha = 0.9\n",
    "summarySet = []\n",
    "while n > 0:\n",
    "    mmr = {}\n",
    "    # kurangkan dengan set summary\n",
    "    for sentence in scores.keys():\n",
    "        if not sentence in summarySet:\n",
    "            mmr[sentence] = alpha * scores[sentence] - (1 - alpha) * calculateSimilarity(sentence, summarySet)  # 公式\n",
    "    selected = max(mmr.items(), key=operator.itemgetter(1))[0]\n",
    "    summarySet.append(selected)\n",
    "    # print (summarySet)\n",
    "    n -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T08:07:48.460529Z",
     "start_time": "2021-02-18T08:07:48.064193Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from itertools import product,count\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "import jieba\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T08:07:48.481051Z",
     "start_time": "2021-02-18T08:07:48.464166Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_similarity(sen1,sen2):\n",
    " \n",
    "    counter = 0\n",
    "    for word in sen1:\n",
    "        if word in sen2:\n",
    "            counter += 1\n",
    "    return counter / (math.log(len(sen1)) + math.log(len(sen2)))\n",
    " \n",
    " \n",
    "def create_graph(word_sent):\n",
    "    num = len(word_sent)\n",
    " \n",
    "    board = [[0.0 for _ in range(num)] for _ in range(num)]\n",
    " \n",
    "    for i , j in product(range(num) , repeat=2):\n",
    "        if i != j:\n",
    "            board[i][j] = calculate_similarity(word_sent[i] , word_sent[j])\n",
    "    return board\n",
    " \n",
    " \n",
    "\"\"\"\n",
    "输入相似度邻接矩阵\n",
    "返回各个句子的分数\n",
    "\"\"\"\n",
    " \n",
    " \n",
    "def weighted_pagerank(weight_graph):\n",
    "    # 把初始的分数值设置为0.5\n",
    "    scores = [0.5 for _ in range(len(weight_graph))]\n",
    "    old_scores = [0.0 for _ in range(len(weight_graph))]\n",
    " \n",
    "    # 开始迭代\n",
    "    while different(scores, old_scores):\n",
    "        for i in range(len(weight_graph)):\n",
    "            old_scores[i] = scores[i]\n",
    " \n",
    "        for i in range(len(weight_graph)):\n",
    "            scores[i] = calculate_score(weight_graph, scores, i)\n",
    "    return scores\n",
    " \n",
    " \n",
    "\"\"\"\n",
    "判断前后分数有没有变化\n",
    "这里认为前后差距小于0.0001\n",
    "分数就趋于稳定\n",
    "\"\"\"\n",
    " \n",
    " \n",
    "def different(scores, old_scores):\n",
    "    flag = False\n",
    "    for i in range(len(scores)):\n",
    "        if math.fabs(scores[i] - old_scores[i]) >= 0.0001:\n",
    "            flag = True\n",
    "            break\n",
    "    return flag\n",
    " \n",
    " \n",
    "\n",
    "# 根据公式求出指定句子的分数\n",
    "def calculate_score(weight_graph, scores, i):\n",
    "    length = len(weight_graph)\n",
    "    d = 0.85\n",
    "    added_score = 0.0\n",
    " \n",
    "    for j in range(length):\n",
    "        fraction = 0.0\n",
    "        denominator = 0.0\n",
    "        # 先计算分子\n",
    "        fraction = weight_graph[j][i] * scores[j]\n",
    "        # 计算分母\n",
    "        for k in range(length):\n",
    "            denominator += weight_graph[j][k]\n",
    "        added_score += fraction / denominator\n",
    "    # 算出最终的分数\n",
    "    weighted_score = (1 - d) + d * added_score\n",
    " \n",
    "    return weighted_score\n",
    " \n",
    " \n",
    "def Summarize(text, n):\n",
    "    # 首先分出句子\n",
    "    #sents = sent_tokenize(text)\n",
    "    for line in text:\n",
    "        sents = re.split('[。？！]',line)[:-1]\n",
    "    # 然后分出单词\n",
    "    # word_sent是一个二维的列表\n",
    "    # word_sent[i]代表的是第i句\n",
    "    # word_sent[i][j]代表的是\n",
    "    # 第i句中的第j个单词\n",
    "    word_sent = [list(jieba.cut(s)) for s in sents]\n",
    "    # print(word_sent)\n",
    " \n",
    "    # 把停用词去除\n",
    "    #for i in range(len(word_sent)):\n",
    "        #for word in word_sent[i]:\n",
    "            #if word in stopwords:\n",
    "                #word_sent[i].remove(word)\n",
    "    similarity_graph = create_graph(word_sent)\n",
    "    scores = weighted_pagerank(similarity_graph)\n",
    "    sent_selected = nlargest(n, zip(scores, count()))\n",
    "    sent_index = []\n",
    "    for i in range(n):\n",
    "        sent_index.append(sent_selected[i][1])\n",
    "    return [sents[i] for i in sent_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T08:15:05.534212Z",
     "start_time": "2021-02-18T08:15:05.408404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "\n",
      "昨天下午，银行负责人称，周先生的账户曾通过网银办理预约转账，虽然银行卡被挂失，但该业务仍会被执行\n",
      "前天，在挂失银行卡10多个小时后，周先生的账户内突然被转账24万元\n",
      "该行行长介绍，经相关部门查询得知，24日，通过网上银行，周先生的账户曾与工行签订预约转账协议，所以按照预约优先的原则，在卡片挂失后，银行仍可按照约定完成转账行为，并将此24万元转入一个招商银行的账户\n"
     ]
    }
   ],
   "source": [
    "texts=['前天，在挂失银行卡10多个小时后，周先生的账户内突然被转账24万元。昨天下午，银行负责人称，周先生的账户曾通过网银办理预约转账，虽然银行卡被挂失，但该业务仍会被执行。银行方面怀疑，这可能是周先生泄露了密码所致。周先生不认同，并于昨天报警。26日上午，周先生前往中国工商银行梨园支行，为一张借记卡办理挂失和取消网银的业务。前天上午10点多，周先生收到银行短信称，在前天凌晨2点多，其已挂失的银行卡账户中，支出（转账）240000元。当天下午，周先生来到工商银行梨园支行询问原因，为什么已经挂失的银行卡账户，仍可以被成功转账？昨天，周先生出示的活期历史明细清单显示，26日，账户内批量存入人民币24万元，前天又被批量转出24万元。周先生介绍，26日打入的24万元，是其此前赎回基金所得的金额。昨天下午，记者以亲友身份陪同周先生来到工商银行梨园支行。该行行长介绍，经相关部门查询得知，24日，通过网上银行，周先生的账户曾与工行签订预约转账协议，所以按照预约优先的原则，在卡片挂失后，银行仍可按照约定完成转账行为，并将此24万元转入一个招商银行的账户。周先生则否认自己曾经在网银上进行预约转账。该行长称，因签订预约转账协议时，需要成功输入网站登录密码和网银密码。银行怀疑，周先生的密码被泄露，被人经其网银签下预约转账协议。']\n",
    "       # texts = [\"7月11日，连续强降雨，让四川登上了中央气象台“头条”，涪江绵阳段水位迅速上涨，洪水一度漫过了宝成铁路涪江大桥桥墩基座，超过封锁水位。洪水在即，中国铁路成都局集团公司紧急调集两列重载货物列车，一前一后开上涪江大桥，每一列货车重量约四千吨，用“重车压梁”的方式，增强桥梁自重，抵御汹涌的洪水。从11日凌晨开始，四川境内成都、绵阳、广元等地连续强降雨，而四川北向出川大动脉—宝成铁路，便主要途径成绵广这一区域。连续的强降雨天气下，绵阳市境内的涪江水位迅速上涨，一度危及到了宝成铁路涪江大桥的安全，上午10时，水位已经超过了涪江大桥上、下行大桥的封锁水位。记者从中国铁路成都局集团公司绵阳工务段了解到，上行线涪江大桥，全长393米，建成于1953年；下行线涪江大桥，全长438米，建成于1995年。“涪江大桥上游有一个水电站，由于洪水太大，水电站已无法发挥调节水位的作用。”情况紧急，铁路部门决定采用“重车压梁”的方式，增强桥梁自重，提高洪峰对桥墩冲刷时的梁体稳定性。简单来说，就是将重量大的货物列车开上涪江大桥，用货车的自重，帮助桥梁抵御汹涌的洪水。恰好，绵阳工务段近期正在进行线路大修，铁路专用的卸砟车，正好停放在绵阳附近。迎着汹涌的洪水，两列重载货车驶向宝成铁路涪江大桥。上午10时30分，第一列46052次货车，从绵阳北站开出进入上行涪江桥。上午11时15分，第二列22001次货车，从皂角铺站进入下行涪江桥。这是两列超过45节编组的重载货物列车，业内称铁路专用卸砟车，俗称“老K车”，车厢里装载的均为铁路道砟，每辆车的砟石的重量在70吨左右。记者从绵阳工务段了解到，货车里满载的砟石、加上一列货车的自重，两列“压桥”的货运列车，每一列的重量超过四千吨。“采用重车压梁的方式来应对水害，在平时的抢险中很少用到。”据了解，在绵阳工务段，上一次采用重车压梁，至少已经是二十年前的事。下午4时许，经铁路部门观测，洪峰过后，涪江水位开始下降，目前已经低于桥梁封锁水位。从下午4点37分开始，两列火车开始撤离涪江大桥。在桥上停留约6个小时后，两列重载货物列车成功完成了“保桥任务”，宝成铁路涪江大桥平安了！\"]\n",
    " \n",
    " \n",
    "print('\\nSummary:\\n')\n",
    "for sentence in Summarize(texts,3):\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
