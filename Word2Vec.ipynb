{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "data = pd.read_csv('clean_xinhuanet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>COMP_NM</th>\n",
       "      <th>NEWS_TITLE</th>\n",
       "      <th>NEWS_TIME</th>\n",
       "      <th>NEWS_SOURCE</th>\n",
       "      <th>NEWS_CONTENT</th>\n",
       "      <th>THE_URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>天津港</td>\n",
       "      <td>【2020·指尖城市】天津港：用技术打造智慧港口新标杆 用智慧赋能港口建设新海蓝</td>\n",
       "      <td>2020-10-28 16:33:16</td>\n",
       "      <td>新华网</td>\n",
       "      <td>新华网天津10月27日电（宫碧莹）2020年，位于渤海湾上海河入海口的天津港建设愈发令人...</td>\n",
       "      <td>http://www.xinhuanet.com/info/2020-10/28/c_139...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>天津港</td>\n",
       "      <td>天津港再添“一带一路”新航线</td>\n",
       "      <td>2020-10-28 10:48:46</td>\n",
       "      <td>新华网</td>\n",
       "      <td>新华社天津10月28日电（记者王井怀）日前，随着“金星佐伊”轮靠泊天津港联盟国际集装箱码...</td>\n",
       "      <td>http://www.xinhuanet.com/fortune/2020-10/28/c_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>中国科技出版传媒股份有限公司</td>\n",
       "      <td>镜观中国·新华社国内新闻照片一周精选</td>\n",
       "      <td>2020-11-28 19:20:43</td>\n",
       "      <td>新华网</td>\n",
       "      <td>雪后长城\\r\\n　　11月21日，北京出现降雪，雪后的慕田峪长城银装素裹，云雾缭绕，美如...</td>\n",
       "      <td>http://www.xinhuanet.com/2020-11/28/c_11267981...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>中国科技出版传媒股份有限公司</td>\n",
       "      <td>金典“夺冠”，安慕希“售罄”！伊利电商双11稳居行业榜首</td>\n",
       "      <td>2020-11-12 16:51:49</td>\n",
       "      <td>新华网</td>\n",
       "      <td>今年双11期间，乳业表现尤为活跃，成为电商销售最火热的品类之一。星图数据显示，11月11...</td>\n",
       "      <td>http://www.xinhuanet.com/food/2020-11/12/c_112...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>中国科技出版传媒股份有限公司</td>\n",
       "      <td>第三十届中国新闻奖获奖作品目录（348件）</td>\n",
       "      <td>2020-11-02 23:52:17</td>\n",
       "      <td>新华网</td>\n",
       "      <td>新华社北京11月2日电\\r\\n第三十届中国新闻奖获奖作品目录（348件）\\r\\n　　特别...</td>\n",
       "      <td>http://www.xinhuanet.com/2020-11/02/c_11266892...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>万华化学</td>\n",
       "      <td>从“华山一条路”走出的“一体化”化学王国——新时代新国企万华改革创新启示录（中）</td>\n",
       "      <td>2019-08-13 10:52:14</td>\n",
       "      <td>经济参考报</td>\n",
       "      <td>万华烟台工业园装置图。资料照片\\r\\n\\r\\n　　在烟台万华化学集团中央研究院，工作人员在做...</td>\n",
       "      <td>http://www.xinhuanet.com/2019-08/13/c_11248692...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>新世界</td>\n",
       "      <td>“云”上生活——从世界VR产业大会云峰会品味生活新场景</td>\n",
       "      <td>2020-10-19 20:18:16</td>\n",
       "      <td>新华网</td>\n",
       "      <td>新华社南昌10月19日电（记者郭杰文）19日，2020世界VR产业大会云峰会在江西南昌开...</td>\n",
       "      <td>http://www.xinhuanet.com/2020-10/19/c_11266311...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>新世界</td>\n",
       "      <td>科普：量子是什么？从“微观世界规律”到人类“新物理革命”</td>\n",
       "      <td>2020-10-19 20:13:15</td>\n",
       "      <td>新华网</td>\n",
       "      <td>新华社合肥10月19日电 题：科普：量子是什么？从“微观世界规律”到人类“新物理革命”\\...</td>\n",
       "      <td>http://www.xinhuanet.com/2020-10/19/c_11266311...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>新世界</td>\n",
       "      <td>新华时评：中国经济正增长为世界注入正能量</td>\n",
       "      <td>2020-10-19 18:04:50</td>\n",
       "      <td>新华网</td>\n",
       "      <td>新华社北京10月19日电 题：中国经济正增长为世界注入正能量\\r\\n　　新华社记者刘红霞...</td>\n",
       "      <td>http://www.xinhuanet.com/fortune/2020-10/19/c_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>罗平锌电</td>\n",
       "      <td>被点名通报后 云南罗平锌电已投入1.9亿整改环境隐患</td>\n",
       "      <td>2019-02-21 18:31:42</td>\n",
       "      <td>新华网</td>\n",
       "      <td>新华社昆明2月21日电（记者 白靖利）“再过几个月，这里将变成一个美丽的运动场。”面对现...</td>\n",
       "      <td>http://www.xinhuanet.com/local/2019-02/21/c_11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>科大讯飞</td>\n",
       "      <td>科大讯飞发布智能录音笔SR502</td>\n",
       "      <td>2020-12-01 15:53:53</td>\n",
       "      <td>新华网</td>\n",
       "      <td>12月1日，科大讯飞线上发布新一代讯飞智能录音笔SR502。\\r\\n\\r\\n　　作为讯飞...</td>\n",
       "      <td>http://www.xinhuanet.com/tech/2020-12/01/c_112...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>科大讯飞</td>\n",
       "      <td>科大讯飞：智能化如何重塑酒店服务及运营流程</td>\n",
       "      <td>2020-11-27 17:46:56</td>\n",
       "      <td>东方网</td>\n",
       "      <td>11月26-27日，2020年环球旅讯峰会及中国住宿业峰会在上海国家会议中心举办，来自住...</td>\n",
       "      <td>http://www.xinhuanet.com/tech/2020-11/27/c_112...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>科大讯飞</td>\n",
       "      <td>【提高上市公司质量在行动】科大讯飞：敬畏市场规范运营 促进企业高质量发展</td>\n",
       "      <td>2020-11-12 15:30:35</td>\n",
       "      <td>新华网</td>\n",
       "      <td>编者按：三十载岁月，中国资本市场从无到有，从小到大，破浪前行，如今A股已跻身全球第二大证...</td>\n",
       "      <td>http://www.xinhuanet.com/finance/2020-11/12/c_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>25</td>\n",
       "      <td>科大讯飞</td>\n",
       "      <td>九城市与中国商飞、腾讯、恒大等头部企业签约合作 G60科创走廊“朋友圈”扩容</td>\n",
       "      <td>2020-11-09 10:29:52</td>\n",
       "      <td>浙江日报</td>\n",
       "      <td>11月8日，长三角G60科创走廊以一体化高质量发展促进国内国际双循环政策发布会在第三届进...</td>\n",
       "      <td>http://www.zj.xinhuanet.com/2020-11/09/c_11267...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>26</td>\n",
       "      <td>科大讯飞</td>\n",
       "      <td>科大讯飞再添A.I.学习新品：讯飞扫描词典笔正式发布</td>\n",
       "      <td>2020-10-27 15:08:13</td>\n",
       "      <td>新华网</td>\n",
       "      <td>10月26日，科大讯飞在官方抖音直播间发布了一款A.I.学习方向的智能硬件产品——讯飞扫...</td>\n",
       "      <td>http://www.xinhuanet.com/tech/2020-10/27/c_112...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>27</td>\n",
       "      <td>万华化学</td>\n",
       "      <td>万华化学9.25亿元收购瑞典国际化工100%股权</td>\n",
       "      <td>2019-07-31 14:36:26</td>\n",
       "      <td>新华网</td>\n",
       "      <td>新华社济南7月31日电（记者王阳）万华化学集团股份有限公司7月31日发布公告称，万华化学...</td>\n",
       "      <td>http://www.xinhuanet.com/2019-07/31/c_11248205...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>29</td>\n",
       "      <td>凯迪生态环境科技股份有限公司</td>\n",
       "      <td>山西、海南、宁夏、新疆生产建设兵团等地“走新”又“走心”宣讲党的十九届五中全会精神</td>\n",
       "      <td>2020-12-15 09:04:54</td>\n",
       "      <td>新华网</td>\n",
       "      <td>12月10日，大同市新时代强音宣讲团成员贾万新在大同市大齿社区宣讲。新华社记者 王菲菲 ...</td>\n",
       "      <td>http://www.xinhuanet.com/2020-12/15/c_11268611...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>31</td>\n",
       "      <td>浦东金桥</td>\n",
       "      <td>最高检发布6起正当防卫不捕不诉典型案例</td>\n",
       "      <td>2020-11-27 17:26:24</td>\n",
       "      <td>新华网</td>\n",
       "      <td>11月27日，最高人民检察院发布6起正当防卫不捕不诉典型案例，进一步明确正当防卫制度的法律适...</td>\n",
       "      <td>http://www.xinhuanet.com/legal/2020-11/27/c_11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>32</td>\n",
       "      <td>南京埃斯顿自动化股份有限公司</td>\n",
       "      <td>“经典传承”全国少儿国画大展颁奖晚会举行</td>\n",
       "      <td>2020-12-08 11:03:39</td>\n",
       "      <td>新华网</td>\n",
       "      <td>12月6日，由书画频道、中国艺术档案学会、中国移动咪咕公司主办，中国美术家协会少儿美术艺...</td>\n",
       "      <td>http://www.xinhuanet.com/shuhua/2020-12/08/c_1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>33</td>\n",
       "      <td>维维股份</td>\n",
       "      <td>维维股份：以现代科技提升粮仓水平，为保粮食安全做贡献</td>\n",
       "      <td>2020-05-22 15:34:58</td>\n",
       "      <td>新华网</td>\n",
       "      <td>在正在召开的2020年“两会”上，国务院总理李克强在22日作政府工作报告时进一步强调了中...</td>\n",
       "      <td>http://www.xinhuanet.com/enterprise/2020-05/22...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID         COMP_NM                                 NEWS_TITLE  \\\n",
       "0    1             天津港   【2020·指尖城市】天津港：用技术打造智慧港口新标杆 用智慧赋能港口建设新海蓝   \n",
       "1    3             天津港                             天津港再添“一带一路”新航线   \n",
       "2    6  中国科技出版传媒股份有限公司                         镜观中国·新华社国内新闻照片一周精选   \n",
       "3    7  中国科技出版传媒股份有限公司               金典“夺冠”，安慕希“售罄”！伊利电商双11稳居行业榜首   \n",
       "4    8  中国科技出版传媒股份有限公司                      第三十届中国新闻奖获奖作品目录（348件）   \n",
       "5   10            万华化学   从“华山一条路”走出的“一体化”化学王国——新时代新国企万华改革创新启示录（中）   \n",
       "6   12             新世界                “云”上生活——从世界VR产业大会云峰会品味生活新场景   \n",
       "7   14             新世界               科普：量子是什么？从“微观世界规律”到人类“新物理革命”   \n",
       "8   17             新世界                       新华时评：中国经济正增长为世界注入正能量   \n",
       "9   19            罗平锌电                 被点名通报后 云南罗平锌电已投入1.9亿整改环境隐患   \n",
       "10  21            科大讯飞                           科大讯飞发布智能录音笔SR502   \n",
       "11  22            科大讯飞                      科大讯飞：智能化如何重塑酒店服务及运营流程   \n",
       "12  23            科大讯飞       【提高上市公司质量在行动】科大讯飞：敬畏市场规范运营 促进企业高质量发展   \n",
       "13  25            科大讯飞     九城市与中国商飞、腾讯、恒大等头部企业签约合作 G60科创走廊“朋友圈”扩容   \n",
       "14  26            科大讯飞                 科大讯飞再添A.I.学习新品：讯飞扫描词典笔正式发布   \n",
       "15  27            万华化学                   万华化学9.25亿元收购瑞典国际化工100%股权   \n",
       "16  29  凯迪生态环境科技股份有限公司  山西、海南、宁夏、新疆生产建设兵团等地“走新”又“走心”宣讲党的十九届五中全会精神   \n",
       "17  31            浦东金桥                        最高检发布6起正当防卫不捕不诉典型案例   \n",
       "18  32  南京埃斯顿自动化股份有限公司                       “经典传承”全国少儿国画大展颁奖晚会举行   \n",
       "19  33            维维股份                 维维股份：以现代科技提升粮仓水平，为保粮食安全做贡献   \n",
       "\n",
       "              NEWS_TIME NEWS_SOURCE  \\\n",
       "0   2020-10-28 16:33:16         新华网   \n",
       "1   2020-10-28 10:48:46         新华网   \n",
       "2   2020-11-28 19:20:43         新华网   \n",
       "3   2020-11-12 16:51:49         新华网   \n",
       "4   2020-11-02 23:52:17         新华网   \n",
       "5   2019-08-13 10:52:14       经济参考报   \n",
       "6   2020-10-19 20:18:16         新华网   \n",
       "7   2020-10-19 20:13:15         新华网   \n",
       "8   2020-10-19 18:04:50         新华网   \n",
       "9   2019-02-21 18:31:42         新华网   \n",
       "10  2020-12-01 15:53:53         新华网   \n",
       "11  2020-11-27 17:46:56         东方网   \n",
       "12  2020-11-12 15:30:35         新华网   \n",
       "13  2020-11-09 10:29:52        浙江日报   \n",
       "14  2020-10-27 15:08:13         新华网   \n",
       "15  2019-07-31 14:36:26         新华网   \n",
       "16  2020-12-15 09:04:54         新华网   \n",
       "17  2020-11-27 17:26:24         新华网   \n",
       "18  2020-12-08 11:03:39         新华网   \n",
       "19  2020-05-22 15:34:58         新华网   \n",
       "\n",
       "                                         NEWS_CONTENT  \\\n",
       "0   　　新华网天津10月27日电（宫碧莹）2020年，位于渤海湾上海河入海口的天津港建设愈发令人...   \n",
       "1   　　新华社天津10月28日电（记者王井怀）日前，随着“金星佐伊”轮靠泊天津港联盟国际集装箱码...   \n",
       "2   　　雪后长城\\r\\n　　11月21日，北京出现降雪，雪后的慕田峪长城银装素裹，云雾缭绕，美如...   \n",
       "3   　　今年双11期间，乳业表现尤为活跃，成为电商销售最火热的品类之一。星图数据显示，11月11...   \n",
       "4   　　新华社北京11月2日电\\r\\n第三十届中国新闻奖获奖作品目录（348件）\\r\\n　　特别...   \n",
       "5   万华烟台工业园装置图。资料照片\\r\\n\\r\\n　　在烟台万华化学集团中央研究院，工作人员在做...   \n",
       "6   　　新华社南昌10月19日电（记者郭杰文）19日，2020世界VR产业大会云峰会在江西南昌开...   \n",
       "7   　　新华社合肥10月19日电 题：科普：量子是什么？从“微观世界规律”到人类“新物理革命”\\...   \n",
       "8   　　新华社北京10月19日电 题：中国经济正增长为世界注入正能量\\r\\n　　新华社记者刘红霞...   \n",
       "9   　　新华社昆明2月21日电（记者 白靖利）“再过几个月，这里将变成一个美丽的运动场。”面对现...   \n",
       "10  　　12月1日，科大讯飞线上发布新一代讯飞智能录音笔SR502。\\r\\n\\r\\n　　作为讯飞...   \n",
       "11  　　11月26-27日，2020年环球旅讯峰会及中国住宿业峰会在上海国家会议中心举办，来自住...   \n",
       "12  　　编者按：三十载岁月，中国资本市场从无到有，从小到大，破浪前行，如今A股已跻身全球第二大证...   \n",
       "13  　　11月8日，长三角G60科创走廊以一体化高质量发展促进国内国际双循环政策发布会在第三届进...   \n",
       "14  　　10月26日，科大讯飞在官方抖音直播间发布了一款A.I.学习方向的智能硬件产品——讯飞扫...   \n",
       "15  　　新华社济南7月31日电（记者王阳）万华化学集团股份有限公司7月31日发布公告称，万华化学...   \n",
       "16  　　12月10日，大同市新时代强音宣讲团成员贾万新在大同市大齿社区宣讲。新华社记者 王菲菲 ...   \n",
       "17  11月27日，最高人民检察院发布6起正当防卫不捕不诉典型案例，进一步明确正当防卫制度的法律适...   \n",
       "18  　　12月6日，由书画频道、中国艺术档案学会、中国移动咪咕公司主办，中国美术家协会少儿美术艺...   \n",
       "19  　　在正在召开的2020年“两会”上，国务院总理李克强在22日作政府工作报告时进一步强调了中...   \n",
       "\n",
       "                                              THE_URL  \n",
       "0   http://www.xinhuanet.com/info/2020-10/28/c_139...  \n",
       "1   http://www.xinhuanet.com/fortune/2020-10/28/c_...  \n",
       "2   http://www.xinhuanet.com/2020-11/28/c_11267981...  \n",
       "3   http://www.xinhuanet.com/food/2020-11/12/c_112...  \n",
       "4   http://www.xinhuanet.com/2020-11/02/c_11266892...  \n",
       "5   http://www.xinhuanet.com/2019-08/13/c_11248692...  \n",
       "6   http://www.xinhuanet.com/2020-10/19/c_11266311...  \n",
       "7   http://www.xinhuanet.com/2020-10/19/c_11266311...  \n",
       "8   http://www.xinhuanet.com/fortune/2020-10/19/c_...  \n",
       "9   http://www.xinhuanet.com/local/2019-02/21/c_11...  \n",
       "10  http://www.xinhuanet.com/tech/2020-12/01/c_112...  \n",
       "11  http://www.xinhuanet.com/tech/2020-11/27/c_112...  \n",
       "12  http://www.xinhuanet.com/finance/2020-11/12/c_...  \n",
       "13  http://www.zj.xinhuanet.com/2020-11/09/c_11267...  \n",
       "14  http://www.xinhuanet.com/tech/2020-10/27/c_112...  \n",
       "15  http://www.xinhuanet.com/2019-07/31/c_11248205...  \n",
       "16  http://www.xinhuanet.com/2020-12/15/c_11268611...  \n",
       "17  http://www.xinhuanet.com/legal/2020-11/27/c_11...  \n",
       "18  http://www.xinhuanet.com/shuhua/2020-12/08/c_1...  \n",
       "19  http://www.xinhuanet.com/enterprise/2020-05/22...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.reset_index().drop('index',axis=1,inplace=False)\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>COMP_NM</th>\n",
       "      <th>NEWS_TITLE</th>\n",
       "      <th>NEWS_TIME</th>\n",
       "      <th>NEWS_SOURCE</th>\n",
       "      <th>NEWS_CONTENT</th>\n",
       "      <th>THE_URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>天津港</td>\n",
       "      <td>【2020·指尖城市】天津港：用技术打造智慧港口新标杆 用智慧赋能港口建设新海蓝</td>\n",
       "      <td>2020-10-28 16:33:16</td>\n",
       "      <td>新华网</td>\n",
       "      <td>新华网天津10月27日电（宫碧莹）2020年，位于渤海湾上海河入海口的天津港建设愈发令人...</td>\n",
       "      <td>http://www.xinhuanet.com/info/2020-10/28/c_139...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>天津港</td>\n",
       "      <td>天津港再添“一带一路”新航线</td>\n",
       "      <td>2020-10-28 10:48:46</td>\n",
       "      <td>新华网</td>\n",
       "      <td>新华社天津10月28日电（记者王井怀）日前，随着“金星佐伊”轮靠泊天津港联盟国际集装箱码...</td>\n",
       "      <td>http://www.xinhuanet.com/fortune/2020-10/28/c_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>中国科技出版传媒股份有限公司</td>\n",
       "      <td>镜观中国·新华社国内新闻照片一周精选</td>\n",
       "      <td>2020-11-28 19:20:43</td>\n",
       "      <td>新华网</td>\n",
       "      <td>雪后长城\\r\\n　　11月21日，北京出现降雪，雪后的慕田峪长城银装素裹，云雾缭绕，美如...</td>\n",
       "      <td>http://www.xinhuanet.com/2020-11/28/c_11267981...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>中国科技出版传媒股份有限公司</td>\n",
       "      <td>金典“夺冠”，安慕希“售罄”！伊利电商双11稳居行业榜首</td>\n",
       "      <td>2020-11-12 16:51:49</td>\n",
       "      <td>新华网</td>\n",
       "      <td>今年双11期间，乳业表现尤为活跃，成为电商销售最火热的品类之一。星图数据显示，11月11...</td>\n",
       "      <td>http://www.xinhuanet.com/food/2020-11/12/c_112...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>中国科技出版传媒股份有限公司</td>\n",
       "      <td>第三十届中国新闻奖获奖作品目录（348件）</td>\n",
       "      <td>2020-11-02 23:52:17</td>\n",
       "      <td>新华网</td>\n",
       "      <td>新华社北京11月2日电\\r\\n第三十届中国新闻奖获奖作品目录（348件）\\r\\n　　特别...</td>\n",
       "      <td>http://www.xinhuanet.com/2020-11/02/c_11266892...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10490</td>\n",
       "      <td>16688</td>\n",
       "      <td>红塔证券</td>\n",
       "      <td>云南师范大学商学院携手红塔证券共育金融人才\\n\\r\\n 云南师范大学商学院携手红塔证券共育金融人才</td>\n",
       "      <td>2020-12-09 16:37:36</td>\n",
       "      <td>云南师范大学商学院</td>\n",
       "      <td>9日，《关于加强云南辖区证券期货知识普及教育的合作备忘录》宣导会暨红塔证券-云南师范大学...</td>\n",
       "      <td>http://www.yn.xinhuanet.com/edu/2020-12/09/c_1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10491</td>\n",
       "      <td>16689</td>\n",
       "      <td>红塔证券</td>\n",
       "      <td>云南民族大学与红塔证券共建投资者教育实践基地</td>\n",
       "      <td>2019-10-23 09:42:05</td>\n",
       "      <td>云南日报</td>\n",
       "      <td>10月21日，“云南民族大学-红塔证券投资者教育实践基地”签约仪式在红塔证券投资者教育基...</td>\n",
       "      <td>http://www.yn.xinhuanet.com/edu/2019-10/23/c_1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10492</td>\n",
       "      <td>16690</td>\n",
       "      <td>红塔证券</td>\n",
       "      <td>红塔证券A股上市首日涨幅43.93%</td>\n",
       "      <td>2019-07-06 11:23:58</td>\n",
       "      <td>昆明日报</td>\n",
       "      <td>昨日，随着上市锣声的响起，红塔证券在上交所挂牌上市，成功登陆A股资本市场。红塔证券股票发...</td>\n",
       "      <td>http://www.yn.xinhuanet.com/finance/2019-07/06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10493</td>\n",
       "      <td>16691</td>\n",
       "      <td>中国银行股份有限公司</td>\n",
       "      <td>中国银行成功代理华能澜沧江水电股份有限公司发行永续中期票据</td>\n",
       "      <td>2019-08-07 12:04:57</td>\n",
       "      <td>中国银行云南省分行</td>\n",
       "      <td>近日，中国银行代理华能澜沧江水电股份有限公司在银行间市场成功发行该公司2019年度第一期...</td>\n",
       "      <td>http://www.yn.xinhuanet.com/finance/2019-08/07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10494</td>\n",
       "      <td>16692</td>\n",
       "      <td>北京科蓝软件系统股份有限公司</td>\n",
       "      <td>70年前的今天 长沙和平解放！</td>\n",
       "      <td>2019-08-05 09:10:52</td>\n",
       "      <td>长沙晚报</td>\n",
       "      <td>长沙晚报掌上长沙8月5日讯（全媒体记者 任波 周游）1949年8月5日，人民解放军夜间从...</td>\n",
       "      <td>http://www.hn.xinhuanet.com/2019-08/05/c_11248...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10495 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID         COMP_NM  \\\n",
       "0          1             天津港   \n",
       "1          3             天津港   \n",
       "2          6  中国科技出版传媒股份有限公司   \n",
       "3          7  中国科技出版传媒股份有限公司   \n",
       "4          8  中国科技出版传媒股份有限公司   \n",
       "...      ...             ...   \n",
       "10490  16688            红塔证券   \n",
       "10491  16689            红塔证券   \n",
       "10492  16690            红塔证券   \n",
       "10493  16691      中国银行股份有限公司   \n",
       "10494  16692  北京科蓝软件系统股份有限公司   \n",
       "\n",
       "                                              NEWS_TITLE            NEWS_TIME  \\\n",
       "0               【2020·指尖城市】天津港：用技术打造智慧港口新标杆 用智慧赋能港口建设新海蓝  2020-10-28 16:33:16   \n",
       "1                                         天津港再添“一带一路”新航线  2020-10-28 10:48:46   \n",
       "2                                     镜观中国·新华社国内新闻照片一周精选  2020-11-28 19:20:43   \n",
       "3                           金典“夺冠”，安慕希“售罄”！伊利电商双11稳居行业榜首  2020-11-12 16:51:49   \n",
       "4                                  第三十届中国新闻奖获奖作品目录（348件）  2020-11-02 23:52:17   \n",
       "...                                                  ...                  ...   \n",
       "10490  云南师范大学商学院携手红塔证券共育金融人才\\n\\r\\n 云南师范大学商学院携手红塔证券共育金融人才  2020-12-09 16:37:36   \n",
       "10491                             云南民族大学与红塔证券共建投资者教育实践基地  2019-10-23 09:42:05   \n",
       "10492                                 红塔证券A股上市首日涨幅43.93%  2019-07-06 11:23:58   \n",
       "10493                      中国银行成功代理华能澜沧江水电股份有限公司发行永续中期票据  2019-08-07 12:04:57   \n",
       "10494                                    70年前的今天 长沙和平解放！  2019-08-05 09:10:52   \n",
       "\n",
       "      NEWS_SOURCE                                       NEWS_CONTENT  \\\n",
       "0             新华网  　　新华网天津10月27日电（宫碧莹）2020年，位于渤海湾上海河入海口的天津港建设愈发令人...   \n",
       "1             新华网  　　新华社天津10月28日电（记者王井怀）日前，随着“金星佐伊”轮靠泊天津港联盟国际集装箱码...   \n",
       "2             新华网  　　雪后长城\\r\\n　　11月21日，北京出现降雪，雪后的慕田峪长城银装素裹，云雾缭绕，美如...   \n",
       "3             新华网  　　今年双11期间，乳业表现尤为活跃，成为电商销售最火热的品类之一。星图数据显示，11月11...   \n",
       "4             新华网  　　新华社北京11月2日电\\r\\n第三十届中国新闻奖获奖作品目录（348件）\\r\\n　　特别...   \n",
       "...           ...                                                ...   \n",
       "10490   云南师范大学商学院  　　9日，《关于加强云南辖区证券期货知识普及教育的合作备忘录》宣导会暨红塔证券-云南师范大学...   \n",
       "10491        云南日报  　　10月21日，“云南民族大学-红塔证券投资者教育实践基地”签约仪式在红塔证券投资者教育基...   \n",
       "10492        昆明日报  　　昨日，随着上市锣声的响起，红塔证券在上交所挂牌上市，成功登陆A股资本市场。红塔证券股票发...   \n",
       "10493   中国银行云南省分行  　　近日，中国银行代理华能澜沧江水电股份有限公司在银行间市场成功发行该公司2019年度第一期...   \n",
       "10494        长沙晚报  　　长沙晚报掌上长沙8月5日讯（全媒体记者 任波 周游）1949年8月5日，人民解放军夜间从...   \n",
       "\n",
       "                                                 THE_URL  \n",
       "0      http://www.xinhuanet.com/info/2020-10/28/c_139...  \n",
       "1      http://www.xinhuanet.com/fortune/2020-10/28/c_...  \n",
       "2      http://www.xinhuanet.com/2020-11/28/c_11267981...  \n",
       "3      http://www.xinhuanet.com/food/2020-11/12/c_112...  \n",
       "4      http://www.xinhuanet.com/2020-11/02/c_11266892...  \n",
       "...                                                  ...  \n",
       "10490  http://www.yn.xinhuanet.com/edu/2020-12/09/c_1...  \n",
       "10491  http://www.yn.xinhuanet.com/edu/2019-10/23/c_1...  \n",
       "10492  http://www.yn.xinhuanet.com/finance/2019-07/06...  \n",
       "10493  http://www.yn.xinhuanet.com/finance/2019-08/07...  \n",
       "10494  http://www.hn.xinhuanet.com/2019-08/05/c_11248...  \n",
       "\n",
       "[10495 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop_duplicates(['COMP_NM', 'NEWS_TITLE'], keep = 'first').drop_duplicates(['COMP_NM', 'NEWS_CONTENT'], keep = 'first')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jieba.load_userdict(\"./dict/SogouLabDic.txt\")\n",
    "jieba.load_userdict(\"./dict/THUOCL_caijing.txt\")\n",
    "jieba.load_userdict(\"./dict/dict_pangu.txt\")\n",
    "#jieba.load_userdict(\"./dict/dict_sougou_utf8.txt\")\n",
    "#jieba.load_userdict(\"./dict/dict_tencent_utf8.txt\")\n",
    "\n",
    "jieba.load_userdict(\"./dict/my_dict.txt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = {}.fromkeys([line.rstrip() for line in open('./dict/Stopword.txt','r',encoding='utf8')])\n",
    "def cut_words(text):\n",
    "    seg = jieba.cut(text)\n",
    "    seg = [x for x in seg if x not in stopwords and x not in [' ', '　', '\\r\\n']]\n",
    "    \n",
    "    return ['   '.join(seg)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_data = data.NEWS_CONTENT[:].map(cut_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./dict/' + 'corpus.txt', 'w+', encoding='utf-8') as f:\n",
    "    for sen in cut_data:\n",
    "        f.write(sen[0])  # 读取的方式和写入的方式要一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-27 14:45:14,206:INFO:collecting all words and their counts\n",
      "2021-01-27 14:45:14,210:INFO:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-01-27 14:45:16,922:INFO:collected 185544 word types from a corpus of 5352654 raw words and 536 sentences\n",
      "2021-01-27 14:45:16,924:INFO:Loading a fresh vocabulary\n",
      "2021-01-27 14:45:17,522:INFO:effective_min_count=1 retains 185544 unique words (100% of original 185544, drops 0)\n",
      "2021-01-27 14:45:17,523:INFO:effective_min_count=1 leaves 5352654 word corpus (100% of original 5352654, drops 0)\n",
      "2021-01-27 14:45:18,192:INFO:deleting the raw counts dictionary of 185544 items\n",
      "2021-01-27 14:45:18,198:INFO:sample=0.001 downsamples 5 most-common words\n",
      "2021-01-27 14:45:18,199:INFO:downsampling leaves estimated 5337197 word corpus (99.7% of prior 5352654)\n",
      "2021-01-27 14:45:18,393:INFO:constructing a huffman tree from 185544 words\n",
      "2021-01-27 14:45:26,008:INFO:built huffman tree with maximum node depth 22\n",
      "2021-01-27 14:45:26,547:INFO:estimated required memory for 185544 words and 100 dimensions: 352533600 bytes\n",
      "2021-01-27 14:45:26,547:INFO:resetting layer weights\n",
      "2021-01-27 14:46:04,129:INFO:training model with 3 workers on 185544 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=3\n",
      "2021-01-27 14:46:05,137:INFO:EPOCH 1 - PROGRESS: at 9.14% examples, 487011 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-27 14:46:06,143:INFO:EPOCH 1 - PROGRESS: at 18.66% examples, 496114 words/s, in_qsize 4, out_qsize 1\n",
      "2021-01-27 14:46:07,146:INFO:EPOCH 1 - PROGRESS: at 27.99% examples, 496523 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-27 14:46:08,146:INFO:EPOCH 1 - PROGRESS: at 37.50% examples, 499505 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-27 14:46:09,153:INFO:EPOCH 1 - PROGRESS: at 47.39% examples, 504606 words/s, in_qsize 4, out_qsize 0\n",
      "2021-01-27 14:46:10,180:INFO:EPOCH 1 - PROGRESS: at 57.09% examples, 504689 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-27 14:46:11,200:INFO:EPOCH 1 - PROGRESS: at 67.16% examples, 508011 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-27 14:46:12,212:INFO:EPOCH 1 - PROGRESS: at 77.24% examples, 511006 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-27 14:46:13,235:INFO:EPOCH 1 - PROGRESS: at 87.31% examples, 512663 words/s, in_qsize 6, out_qsize 0\n",
      "2021-01-27 14:46:14,238:INFO:EPOCH 1 - PROGRESS: at 96.46% examples, 510165 words/s, in_qsize 6, out_qsize 0\n",
      "2021-01-27 14:46:14,542:INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-27 14:46:14,556:INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-27 14:46:14,577:INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-27 14:46:14,578:INFO:EPOCH - 1 : training on 5352654 raw words (5337228 effective words) took 10.4s, 510980 effective words/s\n",
      "2021-01-27 14:46:15,591:INFO:EPOCH 2 - PROGRESS: at 9.33% examples, 494245 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-27 14:46:16,612:INFO:EPOCH 2 - PROGRESS: at 17.54% examples, 461472 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-27 14:46:17,617:INFO:EPOCH 2 - PROGRESS: at 27.05% examples, 476319 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-27 14:46:18,626:INFO:EPOCH 2 - PROGRESS: at 36.57% examples, 483224 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-27 14:46:19,640:INFO:EPOCH 2 - PROGRESS: at 45.90% examples, 484992 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-27 14:46:20,646:INFO:EPOCH 2 - PROGRESS: at 55.78% examples, 491667 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-27 14:46:21,658:INFO:EPOCH 2 - PROGRESS: at 65.49% examples, 494629 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-27 14:46:22,660:INFO:EPOCH 2 - PROGRESS: at 75.19% examples, 497463 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-27 14:46:23,669:INFO:EPOCH 2 - PROGRESS: at 85.26% examples, 501393 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-27 14:46:24,677:INFO:EPOCH 2 - PROGRESS: at 95.15% examples, 503681 words/s, in_qsize 6, out_qsize 0\n",
      "2021-01-27 14:46:25,114:INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-27 14:46:25,128:INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-27 14:46:25,130:INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-27 14:46:25,131:INFO:EPOCH - 2 : training on 5352654 raw words (5337326 effective words) took 10.6s, 505889 effective words/s\n",
      "2021-01-27 14:46:26,149:INFO:EPOCH 3 - PROGRESS: at 9.70% examples, 512072 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-27 14:46:27,166:INFO:EPOCH 3 - PROGRESS: at 19.40% examples, 510964 words/s, in_qsize 4, out_qsize 1\n",
      "2021-01-27 14:46:28,188:INFO:EPOCH 3 - PROGRESS: at 27.61% examples, 483691 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-27 14:46:29,205:INFO:EPOCH 3 - PROGRESS: at 37.31% examples, 490164 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-27 14:46:30,221:INFO:EPOCH 3 - PROGRESS: at 46.83% examples, 492286 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-27 14:46:31,224:INFO:EPOCH 3 - PROGRESS: at 55.04% examples, 483321 words/s, in_qsize 6, out_qsize 0\n",
      "2021-01-27 14:46:32,237:INFO:EPOCH 3 - PROGRESS: at 63.25% examples, 476136 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-27 14:46:33,255:INFO:EPOCH 3 - PROGRESS: at 71.64% examples, 471632 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-27 14:46:34,265:INFO:EPOCH 3 - PROGRESS: at 81.53% examples, 477285 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-27 14:46:35,266:INFO:EPOCH 3 - PROGRESS: at 90.86% examples, 479298 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-27 14:46:36,221:INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-27 14:46:36,227:INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-27 14:46:36,246:INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-27 14:46:36,247:INFO:EPOCH - 3 : training on 5352654 raw words (5337062 effective words) took 11.1s, 480349 effective words/s\n",
      "2021-01-27 14:46:37,271:INFO:EPOCH 4 - PROGRESS: at 8.58% examples, 449127 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-27 14:46:38,277:INFO:EPOCH 4 - PROGRESS: at 18.28% examples, 482036 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-27 14:46:39,289:INFO:EPOCH 4 - PROGRESS: at 28.36% examples, 498732 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-27 14:46:40,293:INFO:EPOCH 4 - PROGRESS: at 38.25% examples, 505646 words/s, in_qsize 5, out_qsize 1\n",
      "2021-01-27 14:46:41,293:INFO:EPOCH 4 - PROGRESS: at 48.32% examples, 512088 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-27 14:46:42,315:INFO:EPOCH 4 - PROGRESS: at 58.02% examples, 511338 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-27 14:46:43,317:INFO:EPOCH 4 - PROGRESS: at 68.10% examples, 515057 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-27 14:46:44,321:INFO:EPOCH 4 - PROGRESS: at 78.17% examples, 517616 words/s, in_qsize 4, out_qsize 0\n",
      "2021-01-27 14:46:45,324:INFO:EPOCH 4 - PROGRESS: at 88.06% examples, 518600 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-27 14:46:46,325:INFO:EPOCH 4 - PROGRESS: at 97.95% examples, 519587 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-27 14:46:46,474:INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-27 14:46:46,498:INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-27 14:46:46,504:INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-27 14:46:46,505:INFO:EPOCH - 4 : training on 5352654 raw words (5337088 effective words) took 10.3s, 520384 effective words/s\n",
      "2021-01-27 14:46:47,524:INFO:EPOCH 5 - PROGRESS: at 9.89% examples, 520885 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-27 14:46:48,525:INFO:EPOCH 5 - PROGRESS: at 19.78% examples, 524319 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-27 14:46:49,545:INFO:EPOCH 5 - PROGRESS: at 29.85% examples, 525554 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-27 14:46:50,562:INFO:EPOCH 5 - PROGRESS: at 39.93% examples, 526446 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-27 14:46:51,568:INFO:EPOCH 5 - PROGRESS: at 50.00% examples, 528341 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-27 14:46:52,600:INFO:EPOCH 5 - PROGRESS: at 59.89% examples, 525538 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-27 14:46:53,600:INFO:EPOCH 5 - PROGRESS: at 69.78% examples, 525924 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-27 14:46:54,632:INFO:EPOCH 5 - PROGRESS: at 79.85% examples, 525371 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-27 14:46:55,632:INFO:EPOCH 5 - PROGRESS: at 89.93% examples, 526739 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-27 14:46:56,605:INFO:worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-27 14:46:56,616:INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-27 14:46:56,626:INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-27 14:46:56,627:INFO:EPOCH - 5 : training on 5352654 raw words (5337217 effective words) took 10.1s, 527461 effective words/s\n",
      "2021-01-27 14:46:56,628:INFO:training on a 26763270 raw words (26685921 effective words) took 52.5s, 508332 effective words/s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "import logging\n",
    " \n",
    "##训练word2vec模型\n",
    " \n",
    "# 获取日志信息\n",
    "logging.basicConfig(format='%(asctime)s:%(levelname)s:%(message)s', level=logging.INFO)\n",
    " \n",
    "# 加载分词后的文本，使用的是Text8Corpus类\n",
    " \n",
    "sentences = word2vec.Text8Corpus(r'./dict/corpus.txt')\n",
    " \n",
    "# 训练模型，部分参数如下\n",
    "model = word2vec.Word2Vec(sentences, size=100, hs=1, min_count=1, window=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.73762643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "sim1 = model.similarity(u'下降', u'增长')\n",
    "\n",
    "print(sim1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `similar_by_word` (Method will be removed in 4.0.0, use self.wv.similar_by_word() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "2021-01-27 14:48:54,109:INFO:precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "与合同最相近的3个字的词\n",
      "第三方 0.6213765144348145\n",
      "人民币 0.5058258175849915\n",
      "多渠道 0.5011968016624451\n",
      "发行人 0.49914175271987915\n",
      "投融资 0.48569124937057495\n"
     ]
    }
   ],
   "source": [
    "print(u'与合同最相近的3个字的词')\n",
    "req_count = 5\n",
    "for key in model.similar_by_word(u'信用', topn=100):\n",
    "    if len(key[0]) == 3:\n",
    "        req_count -= 1\n",
    "        print(key[0], key[1])\n",
    "        if req_count == 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextRank+Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "from itertools import product, count\n",
    "from gensim.models import word2vec\n",
    "import numpy as np\n",
    "\n",
    "np.seterr(all='warn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_sentences(sentence):\n",
    "    puns = frozenset(u'。！？')\n",
    "    tmp = []\n",
    "    for ch in sentence:\n",
    "        tmp.append(ch)\n",
    "        if puns.__contains__(ch):\n",
    "            yield ''.join(tmp)\n",
    "            tmp = []\n",
    "    yield ''.join(tmp)\n",
    " \n",
    " \n",
    "# 句子中的stopwords\n",
    "def create_stopwords():\n",
    "    stop_list = [line.strip() for line in open('./dict/Stopword.txt', 'r', encoding='utf-8').readlines()]\n",
    "    return stop_list\n",
    " \n",
    "\n",
    "def two_sentences_similarity(sents_1, sents_2):\n",
    "    '''\n",
    "    计算两个句子的相似性\n",
    "    :param sents_1:\n",
    "    :param sents_2:\n",
    "    :return:\n",
    "    '''\n",
    "    counter = 0\n",
    "    for sent in sents_1:\n",
    "        if sent in sents_2:\n",
    "            counter += 1\n",
    "    return counter / (math.log(len(sents_1) + len(sents_2)))\n",
    " \n",
    "\n",
    "def create_graph(word_sent):\n",
    "    \"\"\"\n",
    "    传入句子链表  返回句子之间相似度的图\n",
    "    :param word_sent:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    num = len(word_sent)\n",
    "    board = [[0.0 for _ in range(num)] for _ in range(num)]\n",
    " \n",
    "    for i, j in product(range(num), repeat=2):\n",
    "        if i != j:\n",
    "            board[i][j] = compute_similarity_by_avg(word_sent[i], word_sent[j])\n",
    "    return board\n",
    " \n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    '''\n",
    "    计算两个向量之间的余弦相似度\n",
    "    :param vec1:\n",
    "    :param vec2:\n",
    "    :return:\n",
    "    '''\n",
    "    tx = np.array(vec1)\n",
    "    ty = np.array(vec2)\n",
    "    cos1 = np.sum(tx * ty)\n",
    "    cos21 = np.sqrt(sum(tx ** 2))\n",
    "    cos22 = np.sqrt(sum(ty ** 2))\n",
    "    cosine_value = cos1 / float(cos21 * cos22)\n",
    "    return cosine_value\n",
    " \n",
    "\n",
    "def compute_similarity_by_avg(sents_1, sents_2):\n",
    "    '''\n",
    "    对两个句子求平均词向量\n",
    "    :param sents_1:\n",
    "    :param sents_2:\n",
    "    :return:\n",
    "    '''\n",
    "    if len(sents_1) == 0 or len(sents_2) == 0:\n",
    "        return 0.0\n",
    "    vec1 = model[sents_1[0]]\n",
    "    for word1 in sents_1[1:]:\n",
    "        vec1 = vec1 + model[word1]\n",
    " \n",
    "    vec2 = model[sents_2[0]]\n",
    "    for word2 in sents_2[1:]:\n",
    "        vec2 = vec2 + model[word2]\n",
    " \n",
    "    similarity = cosine_similarity(vec1 / len(sents_1), vec2 / len(sents_2))\n",
    "    return similarity\n",
    " \n",
    "\n",
    "def calculate_score(weight_graph, scores, i):\n",
    "    \"\"\"\n",
    "    计算句子在图中的分数\n",
    "    :param weight_graph:\n",
    "    :param scores:\n",
    "    :param i:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    length = len(weight_graph)\n",
    "    d = 0.85\n",
    "    added_score = 0.0\n",
    " \n",
    "    for j in range(length):\n",
    "        fraction = 0.0\n",
    "        denominator = 0.0\n",
    "        # 计算分子\n",
    "        fraction = weight_graph[j][i] * scores[j]\n",
    "        # 计算分母\n",
    "        for k in range(length):\n",
    "            denominator += weight_graph[j][k]\n",
    "            if denominator == 0:\n",
    "                denominator = 1\n",
    "        added_score += fraction / denominator\n",
    "    # 算出最终的分数\n",
    "    weighted_score = (1 - d) + d * added_score\n",
    "    return weighted_score\n",
    " \n",
    "\n",
    "def weight_sentences_rank(weight_graph):\n",
    "    '''\n",
    "    输入相似度的图（矩阵)\n",
    "    返回各个句子的分数\n",
    "    :param weight_graph:\n",
    "    :return:\n",
    "    '''\n",
    "    # 初始分数设置为0.5\n",
    "    scores = [0.5 for _ in range(len(weight_graph))]\n",
    "    old_scores = [0.0 for _ in range(len(weight_graph))]\n",
    " \n",
    "    # 开始迭代\n",
    "    while different(scores, old_scores):\n",
    "        for i in range(len(weight_graph)):\n",
    "            old_scores[i] = scores[i]\n",
    "        for i in range(len(weight_graph)):\n",
    "            scores[i] = calculate_score(weight_graph, scores, i)\n",
    "    return scores\n",
    " \n",
    "\n",
    "def different(scores, old_scores):\n",
    "    '''\n",
    "    判断前后分数有无变化\n",
    "    :param scores:\n",
    "    :param old_scores:\n",
    "    :return:\n",
    "    '''\n",
    "    flag = False\n",
    "    for i in range(len(scores)):\n",
    "        if math.fabs(scores[i] - old_scores[i]) >= 0.0001:\n",
    "            flag = True\n",
    "            break\n",
    "    return flag\n",
    " \n",
    "\n",
    "def filter_symbols(sents):\n",
    "    stopwords = create_stopwords() + ['。', ' ', '，', '.', ' ', '　', '\\r\\n']\n",
    "    _sents = []\n",
    "    for sentence in sents:\n",
    "        for word in sentence:\n",
    "            if word in stopwords:\n",
    "                sentence.remove(word)\n",
    "        if sentence:\n",
    "            _sents.append(sentence)\n",
    "    return _sents\n",
    " \n",
    "\n",
    "def filter_model(sents):\n",
    "    _sents = []\n",
    "    for sentence in sents:\n",
    "        for word in sentence:\n",
    "            if word not in model:\n",
    "                sentence.remove(word)\n",
    "        if sentence:\n",
    "            _sents.append(sentence)\n",
    "    return _sents\n",
    " \n",
    "\n",
    "def summarize(text, n):\n",
    "    tokens = cut_sentences(text)\n",
    "    sentences = []\n",
    "    sents = []\n",
    "    for sent in tokens:\n",
    "        sentences.append(sent)\n",
    "        sents.append([word for word in jieba.cut(re.sub(r'[0-9\",，。:%{}…\\s+]', \"\", sent.replace('从', ''))) if word])\n",
    " \n",
    "    sents = filter_symbols(sents)\n",
    "    sents = filter_model(sents)\n",
    "    graph = create_graph(sents)\n",
    " \n",
    "    scores = weight_sentences_rank(graph)\n",
    "    sent_selected = nlargest(n, zip(scores, count()))\n",
    "    sent_index = []\n",
    "    for i in range(n):\n",
    "        sent_index.append(sent_selected[i][1])\n",
    "    return [sentences[i] for i in sent_index]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:163: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "F:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:72: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "F:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:74: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "F:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:76: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "F:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:78: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"word '的' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-5beb5a42c7d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./dict/news.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmyfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmyfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummarize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-84-591df55660e4>\u001b[0m in \u001b[0;36msummarize\u001b[1;34m(text, n)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[0msents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilter_symbols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[0msents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilter_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m     \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweight_sentences_rank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-84-591df55660e4>\u001b[0m in \u001b[0;36mcreate_graph\u001b[1;34m(word_sent)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mproduct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[0mboard\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_similarity_by_avg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_sent\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_sent\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mboard\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-84-591df55660e4>\u001b[0m in \u001b[0;36mcompute_similarity_by_avg\u001b[1;34m(sents_1, sents_2)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[0mvec2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msents_2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msents_2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[0mvec2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvec2\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[0msimilarity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msents_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvec2\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msents_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1445\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1446\u001b[0m                 )\n\u001b[1;32m-> 1447\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1449\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m   1119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1120\u001b[0m         \"\"\"\n\u001b[1;32m-> 1121\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Method will be removed in 4.0.0, use self.wv.__contains__() instead\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, entities)\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m             \u001b[1;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 471\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    466\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word '的' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    with open('./dict/' + 'news.txt', 'w+', encoding='utf-8') as f:\n",
    "        f.write(data['NEWS_CONTENT'][5])\n",
    "    with open(\"./dict/news.txt\", \"r\", encoding='utf-8') as myfile:\n",
    "        text = myfile.read().replace('\\n', '')\n",
    "        print(summarize(text, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
